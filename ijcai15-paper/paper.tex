% \documentclass[letterpaper]{article}

\documentclass{article}
% The file ijcai15.sty is the style file for IJCAI-15 (same as ijcai07.sty).
\usepackage{ijcai15}

% Use the postscript times font!
\usepackage{times}

\usepackage{todonotes}
\newcommand{\nir}{\todo[color=orange!10]}
\newcommand{\hg}{\todo[color=blue!10]}
\newcommand{\mrj}{\todo[inline,color=green!10]}

% \usepackage{aaai}
% \usepackage{times}

\usepackage{helvet}
\usepackage{courier}
\usepackage{url}
\frenchspacing
% \setlength{\pdfpagewidth}{8.5in}
% \setlength{\pdfpageheight}{11in}
\usepackage{enumitem}
\setlist{nosep} % or \setlist{noitemsep} to leave space around whole list


%Table stuff
\usepackage{xcolor,colortbl}
\definecolor{Gray}{gray}{0.85}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\newcolumntype{a}{>{\columncolor{Gray}}c}
\newcolumntype{b}{>{\columncolor{LightCyan}}c}

\newcommand{\mc}[2]{\multicolumn{#1}{a}{#2}}



\newcommand{\tuple}[1]{{\langle #1\rangle}}
\newcommand{\triple}[1]{{\langle #1\rangle}}
\newcommand{\pair}[1]{{\langle #1\rangle}}

\newcommand{\Omit}[1]{}

\newcommand{\propername}[1]{\textsc{#1}}
\newcommand{\eqdef}{\stackrel{\hbox{\tiny{def}}}{=}}
%\newcommand{\IW}{{\textit{IW}}}
%\newcommand{\SIW}{{\textit{SIW}}}
%\newcommand{\ID}{{\textit{ID}}}
%\newcommand{\BRFS}{{\textit{BrFS}}}
%\newcommand{\BFS}{{\textit{\BFS}}}
%\newcommand{\UCT}{{\textit{UCT}}}
\newcommand{\IW}{{\textsc{IW}}}
\newcommand{\SIW}{{\textsct{SIW}}}
\newcommand{\ID}{{\textsc{ID}}}
\newcommand{\BRFS}{{\textsc{BrFS}}}
\newcommand{\BFS}{{\textsc{2BFS}}}
\newcommand{\UCT}{{\textsc{UCT}}}
\newcommand{\MCTS}{{\textsc{MCTS}}}

\newcommand{\numgames}{$54$}

\newcommand{\Alien}{\propername{Alien}}
\newcommand{\Amidar}{\propername{Amidar}}
\newcommand{\Assault}{\propername{Assault}}
\newcommand{\Asterix}{\propername{Asterix}}
\newcommand{\Asteroids}{\propername{Asteroids}}
\newcommand{\Atlantis}{\propername{Atlantis}}
\newcommand{\BankHeist}{\propername{Bank Heist}}
\newcommand{\BattleZone}{\propername{Battle Zone}}
\newcommand{\BeamRider}{\propername{Beam Rider}}
\newcommand{\Berzerk}{\propername{Berzerk}}
\newcommand{\Bowling}{\propername{Bowling}}
\newcommand{\Boxing}{\propername{Boxing}}
\newcommand{\Breakout}{\propername{Breakout}}
\newcommand{\Carnival}{\propername{Carnival}}
\newcommand{\Centipede}{\propername{Centipede}}
\newcommand{\ChopperCommand}{\propername{Chopper Command}}
\newcommand{\CrazyClimber}{\propername{Crazy Climber}}
\newcommand{\DemonAttack}{\propername{Demon Attack}}
\newcommand{\DoubleDunk}{\propername{Double Dunk}}
\newcommand{\ElevatorAction}{\propername{Elevator Action}}
\newcommand{\Enduro}{\propername{Enduro}}
\newcommand{\FishingDerby}{\propername{Fishing Derby}}
\newcommand{\Freeway}{\propername{Freeway}}
\newcommand{\Frostbite}{\propername{Frostbite}}
\newcommand{\Gopher}{\propername{Gopher}}
\newcommand{\Gravitar}{\propername{Gravitar}}
\newcommand{\Hero}{\propername{Hero}}
\newcommand{\IceHockey}{\propername{Ice Hockey}}
\newcommand{\JamesBond}{\propername{James Bond}}
\newcommand{\JourneyEscape}{\propername{Journey Escape}}
\newcommand{\Kangaroo}{\propername{Kangaroo}}
\newcommand{\Krull}{\propername{Krull}}
\newcommand{\KungFuMaster}{\propername{Kung Fu Master}}
\newcommand{\Montezuma}{\propername{Montezuma Revenge}}
\newcommand{\MsPacman}{\propername{Ms Pacman}}
\newcommand{\NameThisGame}{\propername{Name This Game}}
\newcommand{\Pong}{\propername{Pong}}
\newcommand{\Pooyan}{\propername{Pooyan}}
\newcommand{\PrivateEye}{\propername{Private Eye}}
\newcommand{\Qbert}{\propername{Q*Bert}}
\newcommand{\RiverRaid}{\propername{Riverraid}}
\newcommand{\RoadRunner}{\propername{Road Runner}}
\newcommand{\RobotTank}{\propername{Robot Tank}}
\newcommand{\Seaquest}{\propername{Seaquest}}
\newcommand{\Skiing}{\propername{Skiing}}
\newcommand{\SpaceInvaders}{\propername{Space Invaders}}
\newcommand{\StarGunner}{\propername{Star Gunner}}
\newcommand{\Tennis}{\propername{Tennis}}
\newcommand{\TimePilot}{\propername{Time Pilot}}
\newcommand{\Tutankham}{\propername{Tutankham}}
\newcommand{\UpNDown}{\propername{Up And Down}}
\newcommand{\Venture}{\propername{Venture}}
\newcommand{\VideoPinball}{\propername{Video Pinball}}
\newcommand{\WizardOfWor}{\propername{Wizard Of Wor}}
\newcommand{\Zaxxon}{\propername{Zaxxon}}

\newcommand{\Deepmind}{\propername{Deepmind}}


% AAAI (comment for ECAI)
% \newtheorem{theorem}{Theorem}


 \newtheorem{theorem}{Definition}
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
 \newtheorem{definition}[theorem]{Definition}


\begin{document}
% The file aaai.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%
% \pdfinfo{
% /Title (Classical Planning Algorithms on the Atari Video Games)
% /Author (Nir Lipovetzky, Miquel Ramirez, Hector Geffner)}

\setcounter{secnumdepth}{0}

\title{Classical Planning with Simulators: \\ Results on the Atari Video Games}

\author{\textbf{ID:} 320  \ \ ; \ \ \textbf{Keywords:} classical planning, search, video games}

% \author{Nir Lipovetzky \\
%        The University of Melbourne \\
%     Melbourne, Australia\\
%     {\normalsize\url{nir.lipovetzky@unimelb.edu.au}}
%%% footnote doesn't look nice ; this looks better
%% although can't use Nir's unimelb address that is too long :-)
%% 
%    {\small\url{nirlipo@gmail.com}}
%    \And
%    Miquel Ramirez \\
%    Australian National University \\
%    Canberra, Australia\\
%   {\small\url{miquel.ramirez@gmail.com}}
%    \And
%    Hector Geffner \\
%    ICREA \&  Universitat Pompeu Fabra \\
%    Barcelona, SPAIN \\
%    {\small\url{hector.geffner@upf.edu}}
 %%%%% \thanks{firstname.lastname}
%}


\maketitle

\begin{abstract}
The Atari 2600 games supported in  the Arcade Learning Environment~\cite{bellemare:jair2013}
all feature a known initial (RAM) state and actions that have deterministic effects. Classical planners, however,
cannot be used  off-the-shelf as there is no  compact PDDL-model of  the games, and action effects and goals are  not
known \emph{a priori}. Indeed, there are no explicit goals, and the planner must select actions on-line while interacting 
with a simulator that returns successor states and rewards. None of this precludes  the use of  blind lookahead algorithms 
for action selection like breadth-first search or  Dijkstra's yet such methods are not effective over large  state spaces. 
We thus turn to a different class of  planning methods  introduced recently  that  have been shown to be  effective for solving
large planning problems but which do not require  prior knowledge of state transitions and costs (rewards) or goals.
The simplest such algorithm, called Iterated Width or \IW, consists of  a sequence of calls  $\IW(1)$, $\IW(2)$, \ldots,
where  $\IW(k)$ is  a breadth-first search in which a state is  pruned when it is not the first state in the  search  to make true 
a subset of $k$ atoms. The  empirical results over \numgames\ Atari games show that  $\IW(1)$ performs at the level of UCT, 
the state-of-the-art planning method in this domain, and suggest the potential of width-based methods for planning with
simulators when factored, compact action models are not available.
\end{abstract}

\section{Introduction}

The Arcade Learning Environment (ALE)  provides a challenging  platform for evaluating general, domain-independent AI planners and
learners through a convenient interface to hundreds of Atari 2600 games \cite{bellemare:jair2013}.
Results have been reported so far for basic planning algorithms like breadth-first
search and  \UCT,  reinforcement learning algorithms, and evolutionary  methods \cite{bellemare:jair2013,deep-mind-atari,risto-texas:atari-games}.
The empirical results are impressive in some cases, yet a lot remains to be done, as no method  approaches   the
performance of  human players across a broad range of games.

While all  these games feature a known initial (RAM) state and actions that
have deterministic effects, the problem of selecting the next action
to be done cannot be addressed with off-the-shelf classical planners 
\cite{planning:book,geffner:book}. This is because there is  no compact PDDL-like encoding of the domain and 
the goal to be achieved in each game  is not given. Indeed, there are no goals but rewards, and 
the planner must select actions on-line while interacting with a simulator that just
returns successor states and rewards.
%\footnote{Actually, in the Atari games, the rewards $r(a,s)$ depend only
% on the state $f(a,s)$ that results from doing action $a$ in the state $s$.}

The action selection problem in the Atari games can be seen as a \emph{reinforcement learning} problem
\cite{sutton:book} over a deterministic MDP where the state transitions and rewards are not known, or alternatively,
as a  \emph{net-benefit planning problem} \cite{coles2012survey,keyder:jair09} with unknown state transitions  and  rewards. 
%\Omit{Namely} \Omit{While the \emph{reinforcement learning} problem seeks to synthesize a controller as a policy/function that maximizes 
%the reward of any trajectory\nir{NIR remark}}
While in the former a policy mapping every state $s$ to an action $a$ that maximizes expected reward is sought, in the latter one 
seeks an action sequence $a_0, a_1, \ldots, a_m$ from the  \emph{current}  state $s_0$ that generates
a state sequence $s_0, s_1, \ldots, s_{m+1}$ with  maximum  total reward $\sum_{i=0}^{m} r(a_i,s_i)$,
where $m$ is a sufficiently large planning horizon, and the rewards $r(a_i,s_i)$ and the  state-transitions $s_{i+1}=f(a_i,s_i)$
are known only after  action $a_i$ is applied in the state $s_i$.

The presence of unknown transition and  rewards in  the Atari games does not preclude the use of blind-search
methods like breadth-first search, Dijkstra's algorithm \cite{dijkstra},  or learning methods  such as
LRTA* \cite{korf:lrta}, \UCT\ \cite{uct}, and Q-learning  \cite{sutton:book,bertsekas:neuro}. Indeed,
the net-benefit planning problem with unknown state transitions and rewards over a given planning
horizon, can be mapped into a standard \emph{shortest-path problem} which can be solved optimally by Dijkstra's algorithm.
For this, we just need to  map the unknown rewards $r(a,s)$ into positive (unknown) action costs $c(a,s) = C - r(a,s)$
where  $C$ is a large constant that exceeds the maximum possible reward. The fact that the state
transition and cost functions  $f(a,s)$ and $c(a,s)$ are not known a priori doesn't affect the applicability of
Dijkstra's algorithm, which requires the value of these functions precisely when  the action $a$
is applied in the state $s$.

The limitation of the   basic blind search  methods is that they are not effective  over large state spaces, neither
for solving problems off-line, nor for guiding  a lookahead search for solving problems on-line.
In this work, we thus turn to a recent class of  planning algorithms that combine the scope of blind search
methods with the performance of state-of-the-art classical planners: namely, like ``blind'' search algorithms
they do not require prior knowledge of state transitions, costs, or goals, and yet like heuristic algorithms
they manage to search large  state spaces effectively. The basic algorithm in this class is
called \IW\ for Iterated Width search \cite{nir:ecai2012}. \IW\ consists of a sequence of calls  $\IW(1)$, $\IW(2)$, .., $\IW(k)$, where
$\IW(i)$ is  a  standard breadth-first search where states are pruned right away when they fail to make true some new tuple (set)
of at most $i$ atoms. Namely,   $\IW(1)$ is a breadth-first search that keeps  a state only if  the state is   the first  one
in the search to make some atom  true; $\IW(2)$ keeps a state  only if the state is  the first one to  make  a pair of atoms true, and
so on.  Like plain breadth-first  and iterative deepening searches,  \IW\  is
complete, while searching the state space in a way that makes use of the
\emph{structure of states} given by the values of a finite set of state variables.
In the Atari games, the (RAM) state is given by a vector of 128 bytes, which we associate with 128 variables
$X_i$, $i=1, \ldots, 128$, each of which may take up to $256$ values $x_j$. A state $s$ makes an atom $X_i=x_{j}$
true when the value of the $i$-th byte in the state vector $s$ is $x_j$.
The  empirical results over \numgames\ Atari games show that  $\IW(1)$ performs at the level of UCT, 
the state-of-the-art planning method in this domain, and suggest the potential of width-based methods for planning with
simulators when factored, compact action models are not available.

\Omit{
While a normal, complete breadth-first search runs in time that is exponential in the total number of variables $X_i$,
the procedure $\IW(k)$ runs in time that is exponential in the $k$ parameter, $1 \leq k \leq n$.
Elsewhere, it has been shown that  \IW\ exhibits   state-of-the-art performance on most existing planning benchmark domains
when goals are restricted to single atoms  where \IW\ provably runs in low polynomial time in the number of
variables \cite{nir:ecai2012}. For example, in a blocks world instance with any number of blocks and any initial
configuration,  the goal of  placing a given  block on top of another will be achievable in  quadratic time
in the number of blocks as any such instance can be shown to  have \emph{width} 2. This means  that $\IW(2)$ will
not only be complete then, but also optimal. This is also true for most other benchmark domains in classical planning
that turn out to  have small, bounded widths for \emph{any} instance as long as the goal is atomic.
For dealing with the benchmarks where goals are  not single atoms, e.g., where a given \emph{tower} of blocks needs to be built,
simple extensions of the basic \IW\ procedure have been developed  such as Serialized \IW, where \IW\ is
used to achieve the goals one at at time following a simple goal ordering  \cite{nir:ecai2012,nir:ecai2014}.
The extension of \IW\ for problems where there is no goal but an
additive reward measure  to be optimized, like in the Atari games, is direct.
}

%%  Each iteration IW(k) keeps track of the accumulated reward up to each state $s$ that has been
%% generated but  not pruned, as $R(s) = R(s')+ r(a,s')$ where $s'$ and $a$ are  the single parent state
%% and action $a$ that led to $s$ in IW(k) with $R(s)=0$ for the current state $s$. The best reward

% \Omit{
% The sequence of calls IW(1), IW(2), \ldots, IW(k) manages to combine the scope of ``blind'' search algorithms with the performance of
% heuristic-search algorithms by exploring states  in an  order dictated by the ``width''  parameter $k$;
% states that have width 1 are explored first in linear time, then states that have  width two are explored in quadratic time, and so on.
% In classical planning problems, if the goal has a small width (something common in the classical planning domains when the  goal is a single atom, as
% we have seen above), IW will find goal states  in low polynomial time. Similarly, in the reward-based setting, if  there are states of high reward
% that have small width, IW will find such states in low polynomial time, even if the state space is exponential in the number of variables.
% In this paper, we aim to test whether these ideas work for the Atari games too  when  the RAM state of the emulator
% is used  as the system state.
% }

The paper is organized as follows. We review the iterated width  algorithm and its properties,
look at the    variations of the algorithm that we used in the Atari games, present and analyze the  
experimental results, and discuss related and future work.


\section{Iterated Width}

The Iterated Width  (\IW) algorithm has been introduced as a classical planning algorithm
that takes a planning problem as an input, and computes an action sequence that solves the problem
as the output \cite{nir:ecai2012}. The algorithm however
applies to a broader range of problems. We will characterize such
problems  by means of  a finite and discrete set of states (the state space)
that correspond to vectors of size $n$. Namely, the
states are  \emph{structured} or \emph{factored} , and we take each of the locations in the vector
to represent a variable $X_i$, and the value at that vector location to
represent the value $x_j$ of variable $X_i$ in the state.
In addition to the state space, a problem is defined by an initial state $s_0$,
a set of actions applicable in each state, a
transition function $f$ such that  $s' = f(a,s)$ is
the state that results from applying action $a$  to the state $s$,
and rewards $r(a,s)$ represented by real numbers that result from  applying action $a$ in state $s$.
The transition and reward functions do not need to be known \emph{a priori}, yet in that case, the
state and  reward that results from the application of an action in a state need to be \emph{observable}.
The task is to compute an action sequence $a_0, \ldots, a_m$ for a large horizon $m$
that generates a state sequence $s_0, \ldots, s_{m+1}$ that  maximizes the accumulated
reward $\sum_{i=0}^{m} r(a_i,s_i)$, or that provides a good approximation.

\subsection{The Algorithm}

\IW \ consists of a sequence of calls  $\IW(i)$ for $i=0, 1, 2, \ldots$
over a problem $P$ until a termination condition is reached.
The procedure $\IW(i)$  is a plain forward-state \emph{breadth-first search} with just one change: right after a state
$s$ is generated, the state is  pruned if it doesn't pass a simple \emph{novelty test}.
More precisely,

\begin{itemize}[noitemsep]
\item The \emph{novelty of a newly generate state $s$  in a  search algorithm}
is   $1$ if  $s$ is the first state generated in the search that makes true some  atom $X=x$,
else it is $2$ if  $s$ is the first state that makes a \emph{pair} of atoms $X=x$ and $Y=y$ true,
and so on.

\item  $\IW(i)$ is a breadth-first search that prunes newly generated states when their novelty \Omit{measure}
is greater than $i$.

\item $\IW$  calls $\IW(i)$ sequentially for $i=1,2,\ldots$ until a termination condition is reached,
returning then the best path found.
\end{itemize}

\medskip
For classical planning, the termination condition is the achievement of the goal. In the \emph{on-line setting,}
as in the Atari games, the termination condition is given by a time window or a maximum number of generated nodes.
The  best path found by \IW \ is then the  path  that has a  maximum accumulated reward. The accumulated reward $R(s)$  
of a state $s$ reached in an iteration of \IW \ is determined by the  unique parent state $s'$
and action $a$ leading to $s$ from $s'$ as $R(s) = R(s') + r(a,s')$. The best state is the state $s$ with maximum reward
$R(s)$ generated but not pruned by \IW, and the best path is the one that leads to the state $s$ from the current state.
The action selected  in the on-line setting  is  the first action along  such a path. This action is then executed
and the process repeats from the resulting state. 

%% Like in standard breadth-first search, each iteration IW(i)  prunes
%%% the  duplicate states as well.


\subsection{Performance and Width}

\IW\ is a systematic and complete blind-search algorithm like breadth-first search (\BRFS) and iterative deepening (\ID),
but   unlike these algorithms, it uses the factored representation of the states in terms of variables to structure the
search. This structured exploration  has proved to be very effective over classical planning benchmark domains  when goals are  single atoms.\footnote{
Any conjunctive goal can be mapped into a single dummy atomic goal by adding an action that
achieves the dummy goal and that has the original conjunctive goal as a precondition.
Yet, this changes the definition of the domain.} For example, $37\%$ of the 37,921 problems
considered  in \cite{nir:ecai2012} are solved by $\IW(1)$ while $51.3\%$ are  solved by $\IW(2)$.
These are instances obtained from 37 benchmark domains by splitting problems with $N$ atomic
goals into $N$ problems with one atomic goal each.  Since $\IW(k)$ runs in time that is exponential in $k$, 
this mean that almost $90\%$ of the 37,921 instances are  solved in time that is either linear or quadratic in the number of problem variables, 
which in such encodings are all \emph{boolean}. Furthermore, when the performance of \IW\   is compared with  
a  Greedy Best First Search guided by the additive heuristic $h_{add}$, it turns out that ``blind'' \IW\ solves as many problems as the informed 
search, 34,627 vs. 34,849, far ahead of  other blind search  algorithms like \BRFS\ and \ID\  that solve 9,010 and 8,762 problems each. 
Moreover, \IW\ is  faster and results in shorter plans than in the heuristic search.

The min $k$ value for which $\IW(k)$ solves a problem  is indeed bounded and small in most of
these instances. This is  actually no  accident and  has a \emph{theoretical explanation.} Lipovetzky and Geffner define
a structural parameter called the problem \emph{width} and show that for many of these domains,
\emph{any} solvable instance with atomic goals will  have a bounded and small width that is independent of the number
of variables and states in the problem. The min value $k$ for which the iteration  $\IW(k)$ solves
the problem cannot exceed the problem width, so  the algorithm \emph{\IW\ runs in time and space
that are exponential in the problem width.}

Formally, the \emph{width} $w(P)$ of a problem $P$ is $i$ iff $i$ is the minimum positive integer for which
there is a sequence $t_0, t_1, \ldots, t_n$ of  atom sets  $t_k$  with at most $i$ atoms each, 
such that 1)~$t_0$ is true in the initial state of $P$, 2)~any shortest plan $\pi$  that achieves $t_k$ in $P$
can be extended into a shortest plan that achieves $t_{k+1}$ by extending   $\pi$ with one action, and
3)~any shortest plan that achieves $t_n$ is  a shortest plan for achieving the goal of $P$.

\Omit{
One way to understand this definition is that the problem  width $w(P)$ is at most  $i$ if  there is a
``trail of  stepping stones''  $t_0, t_1, \ldots, t_n$ to reach the problem goal such that
A)~these ``stepping stones''  contain at most $i$ atoms each, B)~each  stepping stone $t_{k+1}$ is
at distance 1 from the previous one $t_k$ when $t_k$ has been reached following the ``trail'',
C)~the ``trail''  preserves ``optimality''; i.e., no  tuple $t_k$ can be reached in less than $k$ steps.
}

While this notion of width and the iterated  width algorithms that are based on it  have been designed
for problems where a goal state needs to be reached, the notions remain
relevant  in  optimization problems as well. Indeed, if a  good path is made of
states $s_i$ each of which has a low width, \IW\ can be made
to find such path in low polynomial time for a small value of the $k$ parameter.
Later on we will discuss  a slight  change  required in \IW\ to enforce this property.
% which is actually  not yet implemented in the code that is tested below.


\section{The Algorithms for the Atari Games}




The number of nodes \emph{generated} by $\IW(1)$ is $n \times D \times b$ in the worst case,
where $n$ is the number of problem variables, $D$ is the size of their domains, and $b$ is the number of
actions per  state.  This same number in a  breadth-first search is not linear in $n$
but exponential. For the Atari games,  $n=128$, $D=256$, and $b=18$, so that the product is
equal to $589,824$, which is large but feasible. On the other hand, the number
of nodes generated by $\IW(2)$ in the worst case is  $(n \times D)^2 \times b$, which is
equal to $19,327,352,832$ which is too large, forcing us to consider only a tiny fraction of
such states. For classical planning problems, the growth in the number of nodes from
$\IW(1)$ to $\IW(2)$ is not that large, as the variables are boolean. Indeed, we could have
taken the state vector for the Atari games as a vector of 1024 boolean variables,
and apply these algorithms to that representation. The results would be different.
In such a case, $\IW(1)$ and $\IW(2)$  would generate $n' \times D' \times b$ and
$(n' \times D')^2 \times b$ states, which for $n'=1024$ and $D=2$
represent  $36,864$ and $75,497,472$ states respectively.
These are feasible numbers, but we haven't used  this representation 
under the assumption that the correlations among the bits in each one of
the 128 words of the state vectors are meaningful.
In summary, from the basic \IW\ algorithm we are testing only the first
call $\IW(1)$.


\IW\ is a purely exploration algorithm that does not take into account  the accumulated
reward for selecting the states to consider. As a simple variant that combines
exploration and exploitation, we evaluated a \emph{best-first search} algorithm
with two queues: one queue  ordered first by  novelty measure
(recall that novelty one means that the state is the first one to make some atom true),
and a second queue ordered by accumulated reward. In one iteration, the best first
search picks up the best node from one queue, and in the second iteration it picks
up the best node from the other queue. This way for combining multiple heuristics
is used in the LAMA planner \cite{richter:lama}, and was introduced in the
planner Fast Downward \cite{helmert:fd}. In addition, we break ties
in the first queue favoring states with largest accumulated reward, and
in the second queue, favoring states with smallest novelty measure. Last, when
a node is expanded, it is removed from the queue, and its children are placed on
both queues. The exception are the nodes with no accumulated reward  that are
placed in the first queue only. We refer to this best-first algorithm
as \BFS.

For  the experiments below, we added two simple variations to  $\IW(1)$ and \BFS.
First, in the breadth-first search underlying $\IW(1)$, we generate the children in
random order. This makes the executions that result from the
$\IW(1)$ lookahead less susceptible to be trapped into loops; a potential
problem in local search algorithms with no memory or learning.
Second, a discount factor $\gamma=0.995$ is used in both algorithms
for discounting future rewards like in \UCT.
For this, each state $s$ keeps its depth $d(s)$ in the search tree,
and if state s' is the child of state $s$ and action $a$,
$R(s')$ is set to $R(s) + \gamma^{d(s)+1} r(a,s)$.
The  discount factor  results in a slight
preference for rewards that can be reached earlier,
which is a  reasonable heuristic in  on-line settings
based on lookahead searches.

\section{Experimental Results}

We tested \IW(1) and \BFS\ over \numgames\  of the $55$ different games 
considered in \cite{bellemare:jair2013}, from now on  abbreviated as BNVB.\footnote{
We left out  \textsc{Skiing} as the reported figures apparently use a  different reward structure.}
%% check
% \footnote{
% Entries marked as  ``--'' in the table are   still running at the time of submission. 
% Each run can take up to $24$ hours of CPU time and there are $10$ runs per game and algorithm. 
% Eventually all of  the  $55$ games will  be included in the evaluation.}
The two  algorithms were used to play the  games in the \emph{on-line planning
setting} supported by ALE where we will compare them with the planning algorithms 
considered by BNVB; namely, breadth-first search and \UCT. ALE supports also a \emph{learning setting} 
where the goal is to learn controllers that map  states into actions without
doing any lookahead. Algorithms across  the two settings are thus not directly comparable
as they compute different things. Learning controllers appears as a more challenging problem
and it is thus not surprising that planning algorithms like \UCT\ tend to achieve a higher score than 
learning algorithms. In addition, the learning algorithms reported by BNVB tend to use 
the state of the screen pixels, while the planning algorithms,  use the state
of the RAM memory. It is not clear however whether one input representation is
more challenging than the other; for the learning algorithms, BNVB mention
that the results tend to be better for the screen inputs. 

Table~\ref{performance} shows the performance of $\IW(1)$ and \BFS\
in comparison  with breadth-first search (\BRFS) and \UCT.
Videos of selected games played by $\IW(1)$, \BFS, and \UCT\ can be 
seen in Youtube.\footnote{\url{http://www.youtube.com/playlist?list=PLXpQcXUQ_CwenUazUivhXyYvjuS6KQOI0}.}
The discount factor used by  all the algorithms is $\gamma=0.995$.
The scores reported for \BRFS\ and \UCT\ are taken from BNVB.  
Our experimental setup  follows theirs except that a maximum budget of  $150,000$ simulated frames 
is applied to $\IW(1)$, \BFS, and \UCT. \UCT\ uses this budget by running
 $500$   rollouts of depth $300$. % \footnote{In \UCT,  $500$ base policy rollouts
% of depth $300$ is equivalent to a budget of $150,000$ simulated frames.}.
The bound on the number of simulated frames is like a bound on
lookahead time, as most of the time in the lookahead  is spent
in calls to the emulator for computing the next RAM state. 
This is why the average time per action is similar to all the algorithms
except $\IW(1)$, that due to its pruning does not always use
the full budget and takes less time per action on average.

Also,  as reported by  BNVB, all of the algorithms 
reuse the frames in the sub-tree of the previous lookahead that is rooted in the selected 
child, deleting its siblings and their descendants. More precisely, no calls 
to the emulator are done for transitions that are cached in that sub-tree, 
and such reused frames are not discounted from the budget that is thus a bound 
on the number of \emph{new}  frames per lookahead. In addition, in $\IW(1)$, the states that 
are reused  from the previous searches are ignored in the computation of the novelty of 
new states so that more states can escape pruning. Otherwise, $\IW(1)$ often
uses a fraction of the  budget.  This is not needed in \BFS\ which does 
no pruning.  $\IW(1)$ and \BFS\ are  limited to  search up to a depth of
$m=1,500$ frames and up to $m=150,000$ frames per root branch. This is
to avoid the search from  going too deep or being too committed to a
single root action. 
% Similarly UCT defines a limit of $m=300$ frames for each rollout. 
% Note that UCT is more likely to have a performance
% loss if the depth is increased, as the number of rollouts would
% decrease in order to maintain the same budget of $150,000$ simulated
% frames. 

Last, in the lookahead, \IW(1) and \BFS\ select an action every $5$ frames, while \UCT\  selects 
an action in every frame.  This means that in order to explore a branch 
$300$ frames deep, UCT gets to choose $300$ actions, while \IW($1$) and \BFS\ 
get to choose $60$ actions, both however using the same $300$ frames from the
budget. For this, we  followed the setup  of \BRFS\ in BNVB 
that also selects actions every $5$ frames, matching the behavior
of the emulator that requests an action also every $5$ frames. Since the 
lookahead budget is given by a maximum number of (new) frames, and the time 
is mostly taken by calls to the emulator, this may not be the best choice  
for $\IW(1)$ and \BFS\ that may therefore not be exploiting all the 
options afforded by the budget. We'll look at this further in the future.

\Omit{
The last difference inherited from BNVB between all algorithms
and \UCT\, is that \UCT\ selects an action randomly in the rollout every
single frame, while the others select an action every 5 frames, and
the remain ones are $no-op$, as is done in the simulation
environment. Therefore, a node in all algorithms represent $5$ frames,
but in UCT represents $1$ frame. As a result, a \UCT\ rollout of depth
$300$ frames involve the selection of 300 actions, decreasing the
budget of simulated frames by $300$, while the other algorithms need a
branch of $1500$ frames to select $300$ actions, decreasing the budget
by $1500$ frames instead.
}


Table~\ref{performance} shows that both $\IW(1)$ and \BFS\ outperform \BRFS, which  rarely collects
reward in  many domains as  the depth of the \BRFS\ search tree results in a lookahead of $0.3$
seconds ($20$ frames or $4$ nodes deep). The notable exception to this
is \Centipede\ where abundant reward can be collected with a shallow
lookahead. On the other hand, both $\IW(1)$ and \BFS\ normally 
reach states that are up to $350$--$1500$ frames deep ($70$--$260$ nodes or 
$6$--$22$ seconds),  even if $\IW(1)$ does not always use all the simulation frames allocated due to its
agressive pruning. This can be observed in games such as \Breakout, \CrazyClimber, \Kangaroo, and \Pooyan, where the average
CPU time for each lookahead is up to $10$ times faster than \BFS.  Computation time for \UCT\ and \BRFS\ are  similar to \BFS, as the
most expensive part of the computation is the  generation of frames through the simulator,  and these three  algorithms 
always use  the full budget.

More interestingly,  \IW(1) outscores \UCT\  in $31$ of the \numgames\ games,
while \BFS\ outscores \UCT\ in $26$.
%\footnote{These numbers can actually
%improve further as 6  games for \IW(1) and 4 games for \BFS\ are still running.}
On the other hand, \UCT\ does better than $\IW(1)$ and \BFS\ in $19$ and $25$ games respectively.
The relative performance between $\IW(1)$ and \BFS\ makes $\IW(1)$  the
best of the two in $34$ games, and  \BFS\  in $16$. In terms of the number of games 
where an algorithm is the best, % $\IW(1)$  domainates the others: 
$\IW(1)$ is  the best in $26$ games, \BFS\ in $13$ games, and \UCT\ in $19$ games. 
Also, \BRFS\ is best in $2$ games (\Centipede, tied up in \Boxing), while the other three algorithms
are tied in another 2 games (\Pong, \Boxing). 
% , and $\IW(1)$ and \BFS\ are tied in \TimePilot.

\Omit{As mentioned in the
previous section, the order in which $\IW(1)$ and \BFS\ generate the
children of an expanded node is randomized, something which has a
potential effect on the novelty measures of nodes, and hence in the
search graph. We found that this randomization helps, in particular in
\Qbert, where not randomizing results in $\IW(1)$ being trapped into
loops sometimes.}


\begin{table*}
\centering
% \addtolength{\tabcolsep}{-2pt}
% {\scriptsize
{\small
\begin{tabular}{@{}l|rr|rr|r|r@{}}
%% table header
% \hline\\[-3ex]
%\hline
\multicolumn{1}{c|}{}& \multicolumn{2}{c|}{$\IW(1)$}& \multicolumn{2}{c|}{\BFS} & \multicolumn{1}{c|}{\BRFS} &  \multicolumn{1}{c}{\UCT}
% \\[.2ex]
\\
% \hline\\\\[-3ex]
\hline
%% table body
\multicolumn{1}{c|}{Game}  &
% \multicolumn{1}{c|}{Avg. Score} &
% \multicolumn{1}{c}{Avg. Score} & \multicolumn{1}{c|}{Avg. Time}  &
% \multicolumn{1}{c}{Avg. Score} & \multicolumn{1}{c|}{Avg. Time}  &
% \multicolumn{1}{c}{Avg. Score} & \multicolumn{1}{c|}{Avg. Time}  &
% \multicolumn{1}{c}{Avg. Score} & \multicolumn{1}{c}{Avg. Time}\\[.2ex]
 \multicolumn{1}{c}{Score} & \multicolumn{1}{c|}{Time}  &
 \multicolumn{1}{c}{Score} & \multicolumn{1}{c|}{Time}  &
 \multicolumn{1}{c|}{Score} &
 \multicolumn{1}{c}{Score} 
% \\[.2ex]
\\[.2ex]
% \hline\\
\hline
\Alien & \mc{1}{\textbf{25634}} & 81 & \mc{1}{12252} & 81 & 784 & 7785 \\ \hline
\Amidar & \mc{1}{\textbf{1377}} & 28 &\mc{1}{ 1090} & 37 & 5 & 180 \\ \hline
\Assault & 953 & 18 & 827 & 25 & 414 & \textbf{1512} \\ \hline
\Asterix & 153400 & 24 & 77200 & 27 & 2136 & \textbf{290700} \\ \hline
\Asteroids & \mc{1}{\textbf{51338}} & 66 & \mc{1}{22168} & 65 & 3127 & 4661 \\ \hline
\Atlantis & 159420 & 13 & 154180 & 71 & 30460 & \textbf{193858} \\ \hline
\BankHeist & \mc{1}{\textbf{717}} & 39 & 362 & 64 & 22 & 498 \\ \hline
\BattleZone & 11600 & 86 &\mc{1}{ \textbf{330800}} & 87 & 6313 & 70333 \\ \hline
\BeamRider &\mc{1}{ 9108} & 23 & \mc{1}{\textbf{9298}} & 29 & 694 & 6625 \\ \hline
\Berzerk & \mc{1}{\textbf{2096}} & 58 & \mc{1}{802} & 73 & 195 & 554 \\ \hline
\Bowling & \mc{1}{\textbf{69}} & 10 & \mc{1}{50} & 60 & 26 & 25 \\ \hline
\Boxing & \mc{1}{\textbf{100}} & 15 &\mc{1}{ \textbf{100}} & 22 & \textbf{100} & \textbf{100} \\ \hline
\Breakout &\mc{1}{ 384} & 4 &\mc{1}{ \textbf{772}} & 39 & 1 & 364 \\ \hline
\Carnival & \mc{1}{\textbf{6372}} & 16 &\mc{1}{ 5516} & 53 & 950 & 5132 \\ \hline
\Centipede & 99207 & 39 & 94236 & 67 & \textbf{125123} & 110422 \\ \hline
\ChopperCommand & 10980 & 76 & 27220 & 73 & 1827 & \textbf{34019} \\ \hline
\CrazyClimber & 36160 & 4 & 36940 & 58 & 37110 & \textbf{98172} \\ \hline
\DemonAttack & 20116 & 33 & 16025 & 41 & 443 & \textbf{28159} \\ \hline
\DoubleDunk & -14 & 41 & 21 & 41 & -19 & \textbf{24} \\ \hline
\ElevatorAction & 13480 & 26 & 10820 & 27 & 730 & \textbf{18100} \\ \hline
\Enduro &\mc{1}{ \textbf{500}} & 66 & \mc{1}{359} & 38 & 1 & 286 \\ \hline
\FishingDerby & 30 & 39 & 6 & 62 & -92 & \textbf{38} \\ \hline
\Freeway &  \mc{1}{\textbf{31}} & 32 & \mc{1}{23} & 61 & 0 & 0 \\ \hline
\Frostbite & \mc{1}{902} & 12 &  \mc{1}{\textbf{2672}} & 38 & 137 & 271 \\ \hline
\Gopher & 18256 & 19 & 15808 & 53 & 1019 & \textbf{20560} \\ \hline
\Gravitar &\mc{1}{ 3920} & 62 &  \mc{1}{\textbf{5980}} & 62 & 395 & 2850 \\ \hline
\Hero &  \mc{1}{\textbf{12985}} & 37 & 11524 & 69 & 1324 & 12860 \\ \hline
\IceHockey &  \mc{1}{\textbf{55}} & 89 &\mc{1}{ 49} & 89 & -9 & 39 \\ \hline
\JamesBond &  \mc{1}{\textbf{23070}} & 0 & \mc{1}{10080} & 30 & 25 & 330 \\ \hline
\JourneyEscape & \mc{1}{40080} & 38 &  \mc{1}{\textbf{40600}} & 67 & 1327 & 7683 \\ \hline
\Kangaroo &  \mc{1}{\textbf{8760}} & 8 &\mc{1}{ 5320} & 31 & 90 & 1990 \\ \hline
\Krull &  \mc{1}{\textbf{6030}} & 28 & 4884 & 42 & 3089 & 5037 \\ \hline
\KungFuMaster &  \mc{1}{\textbf{63780}} & 21 & 42180 & 43 & 12127 & 48855 \\ \hline
\Montezuma & 0 & 14 &  \mc{1}{\textbf{540}} & 39 & 0 & 0 \\ \hline
\MsPacman & 21695 & 21 & 18927 & 23 & 1709 & \textbf{22336} \\ \hline
\NameThisGame & 9354 & 14 & 8304 & 25 & 5699 & \textbf{15410} \\ \hline
\Pong & \mc{1}{\textbf{21}} & 17 &\mc{1}{ \textbf{21}} & 35 & -21 & \textbf{21} \\ \hline
\Pooyan & 11225 & 8 & 10760 & 16 & 910 & \textbf{17763} \\ \hline
\PrivateEye & -99 & 18 & \mc{1}{\textbf{2544}} & 44 & 58 & 100 \\ \hline
\Qbert & 3705 & 11 & 11680 & 35 & 133 & \textbf{17343} \\ \hline
\RiverRaid & \mc{1}{\textbf{5694}} & 18 &\mc{1}{ 5062} & 37 & 2179 & 4449 \\ \hline
\RoadRunner & \mc{1}{\textbf{94940}} & 25 &  \mc{1}{68500} &  41  & 245 & 38725 \\ \hline
\RobotTank &\mc{1}{ \textbf{68}} & 34 &\mc{1}{ 52} & 34 & 2 & 50 \\ \hline
\Seaquest &\mc{1}{ \textbf{14272}} & 25 &\mc{1}{ 6138} & 33 & 288 & 5132 \\ \hline
\SpaceInvaders &\mc{1}{ 2877} & 21 & \mc{1}{\textbf{3974}} & 34 & 112 & 2718 \\ \hline
\StarGunner &\mc{1}{ 1540} & 19 & \mc{1}{\textbf{4660}} & 18 & 1345 & 1207 \\ \hline
\Tennis &\mc{1}{ \textbf{24}} & 21 & \mc{1}{\textbf{24}} & 36 & -24 & 3 \\ \hline
\TimePilot & 35000 & 9 & 36180 & 29 & 4064 & \textbf{63855} \\ \hline
\Tutankham & 172 & 15 & 204 & 34 & 64 & \textbf{226} \\ \hline
\UpNDown &\mc{1}{ \textbf{110036}} & 12 & 54820 & 14 & 746 & 74474 \\ \hline
\Venture & \mc{1}{\textbf{1200}} & 22 & \mc{1}{980} & 35 & 0 & 0 \\ \hline
\VideoPinball & \mc{1}{\textbf{388712}} & 43 & 62075 & 43 & 55567 & 254748 \\ \hline
\WizardOfWor & \mc{1}{\textbf{121060}} & 25 & 81500 & 27 & 3309 & 105500 \\ \hline
\Zaxxon & \mc{1}{\textbf{29240}} & 34 & 15680 & 31 & 0 & 22610 \\ \hline


\multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\ \hline
\# Times Best (\numgames\ games)& \multicolumn{2}{c|}{\textbf{26}} & \multicolumn{2}{c|}{13} & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{19} \\ \hline
\# Times Better than \IW\ (\numgames\ games) & \multicolumn{2}{c|}{--} & \multicolumn{2}{c|}{16} & \multicolumn{1}{c|}{1}  & \multicolumn{1}{c|}{\textbf{19}} \\ \hline
\# Times Better than \BFS\ (\numgames\ games) & \multicolumn{2}{c|}{\textbf{34}} & \multicolumn{2}{c|}{--} & \multicolumn{1}{c|}{1}  & \multicolumn{1}{c|}{25} \\ \hline
\# Times Better than \UCT\ (\numgames\ games) & \multicolumn{2}{c|}{\textbf{31}} & \multicolumn{2}{c|}{26} & \multicolumn{1}{c|}{1}  & \multicolumn{1}{c|}{--} \\ \hline


\end{tabular}
}

 \caption{\small Performance that results from various  lookahead algorithms in \numgames\  Atari 2600 games. The algorithms, \BRFS, $\IW(1)$, \BFS, and \UCT, are evaluated
 over $10$ runs (episodes) for each game. 
The maximum episode duration is  $18,000$  frames and every algorithm is limited to a lookahead budget of 150,000 simulated frames.
Figures for \BRFS\ and \UCT\ taken from Bellemare et al. Average  CPU times per action in seconds, rounded to nearest integer, shown for  $\IW(1)$ and \BFS. 
Numbers  in bold show best performer in terms of average score, while numbers shaded in light grey show scores  that are better than \UCT's. 
Bottom part of the table shows pairwise comparisons among the algorithms.}
\label{performance}
\end{table*}



Likewise, in \Freeway\ and \Berzerk\ both $\IW(1)$ and \BFS\ attain a better
score than the baseline semi-random algorithm \emph{Perturb} in
\cite{bellemare:jair2013}, that beats \UCT\ on those games.
%% to say the 2 lines below must add context: .. eg
%%% ``which in *these games* outperforms both \UCT\ and the reinforcement learning methods.''
%% add it if that's the case ..
\emph{Perturb} is a simple algorithm
that selects  a fixed  action with probability $0.95$, and a random
action with probability $0.05$.  For \emph{Perturb}, BNVB do not report 
the average score  but the  best score.  \emph{Perturb} manages to do well in domains where rewards
are deep but can be reached by  repeating the same action.
This is the case of \Freeway, where a chicken has to run to the top of the screen across a ten lane
highway filled with traffic. Every time the chicken gets across (starting at the bottom), there is one
unit of reward. If the chicken is hit by a car, it goes back some lanes. 
In \Freeway, only $12$ out of the $18$ possible actions have an effect:
$6$ actions move the chicken up (up-right, up-left, up-fire, up-right-fire, up-left-fire), 
$6$ actions move the chicken down (down-right, down-left, down-fire, down-right-fire, down-left-fire), 
and $6$ actions do nothing.  \emph{Perturb}  does well in this domain 
when the selected fixed action moves the chicken up.  As noted 
in Table~\ref{performance} and seen in the provided video, \UCT\ does not manage to take the chicken
across the highway at all. 
%
The reason that \UCT\ does not collect any reward is that it needs to move the chicken up
at least 240 times\footnote{One needs to move the chicken up for at least $4$ seconds ($240$ frames) in order to get it across the 
highway.} something that is very unlikely in a random exploration. $\IW(1)$ does not have this limitation and is best in \Freeway.
%% The reason for \UCT\ to perform erratically 
%% is that the roll-outs do not manage to collect any reward. A sequence of actions 
%% with at least $48$ actions embedded that move the chicken up are required in order 
%% to collect a reward. Since the probability for the roll-out to choose an action that moves the chicken up 
%% is $1/3$, with the parameters chosen by BNVB, the likelihood of \UCT\ generating such a sequence of actions 
%% that \emph{also} avoids incoming traffic in a timely fashion is rather low.
%% %% the previous number was not correct; no-op actions do not count, and thus needs
%% %%% 200 more move up actions than move down actions, although this is not accurate either
%% %%%% as move down actions that take chicken below the screen count as no-ops too ..
%% $\IW(1)$ does not have this limitation and is best in \Freeway.

%%
%%% Sobre texto below; no me convence; no sabemos como cambia el RAM aunque podamos suponer
%%% obvio: si un byte codifica donde esta el chicken en la pantalla .. tendriamos una explicación
%% On the other hand, \IW\ manages to find rewards without exhausting the  budget. The main reason that
%% explains this behaviour is that at most only 3 generated states from
%% the same parent node will pass the novelty test, as in any expanded
%% node there is at most 3 different new possible bytes in the RAM state
%% resulting from moving up, down, or do-nothing.

$\IW(1)$ obtains better scores than the best learning algorithm in
\cite{deep-mind-atari} in the $7$ games considered there, and
\BFS\ does so in $6$ of the $7$ games. Comparing with the scores reported for the reinforcement learning algorithms in BNVB,
we note that both $\IW(1)$ and \BFS\ do much better than the best learning algorithm in
those games where the learning algorithms outperform \UCT\, namely, \Montezuma, \Venture\
and \Bowling. We take this as evidence that $\IW(1)$ and \BFS\ are as at least as good as learning
algorithms at finding rewards in games where \UCT\ is not very effective.

For instance, in \Montezuma\ rewards are very sparse, deep, and most of the actions lead to losing a life with
no immediate penalty or consequence.  In our experiments, all algorithms achieve $0$ score, 
except for \BFS\ that achieves an average score of $540$, and a score
of $2,500$ in one of the runs. This means however that even \BFS\  is not able to 
consistently find rewards in this game. This game and several others
like \Breakout\ and \SpaceInvaders\ could be much simpler by adding
negative rewards for losing a life. We have indeed observed that
our planning algorithms do not care much about losing lives
until there is just one life left, when their play noticeably improves.
This can be seen in the videos mentioned above, and suggest a simple form of learning that would be useful to both planners
and reinforcement learning algorithms.


We are not  reporting  the performance of $\IW(k)$ with parameter $k=2$ because  in our
preliminary tests and according to the discussion in the previous section,
it doesn't  appear to improve much on  \BRFS, even if  it results in a lookahead
that is $5$ times deeper,  but  still too shallow  to compete with the other
planning algorithms.

\section{Exploration and Exploitation}

The notion of width underlying the iterated width algorithm was developed in the context
of classical planning in order to understand why most of the hundreds of existing benchmarks
appear to be relatively simple for current planners, even
though classical planning is PSPACE-complete \cite{bylander:complexity}. A  partial answer is that
most of these domains have a low width, and hence, can be solved in low polynomial time (by \IW)
when goals contain a single atom.  Benchmark problems with multiple atomic goals tend to be
easy too, as the goals can often be achieved one at a time after a simple goal ordering \cite{nir:ecai2012}.

In the iterated width algorithm, the key notion is the \emph{novelty measure} of a state in the underlying
breadth-first search. These novelty measures make use of the factored representation of the states for
providing a structure to the search: states that have width $1$ are explored first in linear time,
then states that have  width $2$ are explored in quadratic time, and so on. In classical planning problems
with atomic goals, this way of organizing the search  pays off both theoretically and practically.

The use of ``novelty measures'' for guiding an optimization search while ignoring the function that is being
optimized is   common to the  novelty-based search methods developed independently
in the context of genetic algorithms \cite{lehman:novelty}. %%% bibtex for this below
In these methods individuals in the population are not ranked according to the  optimization function
but in terms of how ``novel'' they are in relation to the rest of the population, thus encouraging diversity
and exploration rather than (greedy) exploitation. The actual definition of novelty
in such a case is domain-dependent; for example, in the evolution of a controller
for guiding a robot in a maze, an individual controller  will not be ranked by how
close it takes the robot to the goal (the greedy measure), but by the distance between
the locations that are  reachable with it, and the locations reachable with
the  other controllers in the population (a diversity measure). The novelty measure used by 
\IW, on the other hand, is domain-independent and it
is determined by the structure of the states as captured by the problem variables.

The balance between exploration and exploitation has received considerable attention in
reinforcement learning \cite{sutton:book}, where both are required for converging to an
optimal behavior. In the Atari games, as in other deterministic problems, however, ``exploration''
is not needed for optimality purposes, but just for improving the effectiveness of the
 lookahead search. Indeed, a best-first search algorithm guided only  by (discounted)
accumulated reward will deliver eventually  best moves, but it will not be as effective
over small time windows, where like breadth-first search it's likely not to find any rewards
at all. The \UCT\ algorithm provides a method for balancing exploration and exploitation,
which  is  effective over small time windows. The \BFS\ algorithm above with two queues
that alternate, one guided by the novelty measures  and the other by the accumulated reward,
provides a different scheme. The first converges to the optimal behavior asymptotically; the
second in a bounded number of steps, with the caveat below.

% Alternatively, novelty-guided probes, possibly with
% some noise added, could potentially pay off as  a  base policy for \UCT\ in replacement
% of random roll outs, as they apparently manage to exploit the structure of many problems.


\section{Duplicates and Optimality}

The notions of width and the \IW\  algorithm guarantee that  states with low width will be generated
in low polynomial time through \emph{shortest} paths. In the presence of rewards like the Atari games,
however, the  interest is not  in the shortest paths but in the \emph{best} paths; i.e, the paths with
maximum reward.  \IW\ may actually  fail to find such paths even when calling \IW($k$) with a high 
$k$ parameter. Optimality  could be  achieved by replacing the breadth-first search
underlying \IW($k$) by Dijkstra's algorithm yet such a move would make the relation between
\IW\ and the notion of width less clear. A better  option is to change  \IW\ to comply with 
a different property; namely, that if there is a ``rewarding'' path made up of states of 
low width, then \IW\ will find such paths or better ones in time that is exponential 
in their width. For this,  a simple change in \IW\ suffices: when generating a state 
$s$ that is a \emph{duplicate} of a state $s'$ that has been previously generated and not pruned, 
$s'$ is replaced by $s$ if $R(s) > R(s')$, with the change of reward propagated to the descendants of $s'$
that are in memory. This is similar to the change required 
in the  A* search algorithm for preserving optimality when moving from consistent to inconsistent heuristics \cite{pearl:heuristics}. 
The alternative is to ``reopen'' such nodes. The same change is actually needed in \BFS\ to ensure that, 
if given enough time, \BFS\ will actually find optimal paths.
% Changing \BFS\ to account for such paths guarantees that, if given enough time, \BFS\ will actually find
% the optimal paths. 
The code used for \IW\ and \BFS\ in the
experiments above does not implement this change  as the overhead involved in checking for duplicates 
in some test cases did not appear to pay off. More experiments  however are  needed to
find out if this is actually the most effective option.

\section{Summary}

We have shown experimentally that width-based algorithms like $\IW(1)$ and \BFS\
that originate in work in classical planning,  can be used to play  the Atari video
games where they  achieve  state-of-the-art performance. The  results also  suggest 
more generally the potential of width-based methods for planning with simulators when factored, 
compact action models are not available. In this sense, the scope of these planning methods 
is broader than those of heuristic-search planning methods that require propositional
encodings of actions and goals, and with suitable extensions, 
may potentially  approach the scope of \MCTS\ methods like \UCT\ that work on simulators as well. 

\newpage


%% This suggests to us that the algorithms discussed in the 
%% present paper could be applied in a broad variety of settings where only on-line 
%% action selection is feasible or suitable.

\Omit{
Concerning the Atari games, there are two problems in all the planning methods
discussed. First, by using the RAM state of the Atari console one can consider
that they are ``cheating'', as they use information that is not available to the human player. And second, they do not get 
better with experience. In the future, we would like to explore 
whether variations of these algorithms  can be used to play from the information conveyed by the  
state of the screen pixels as opposed to the RAM state, and the potential uses of the new ideas for learning,
such the use of the  novelty measures for shaping rewards.

\medskip

\noindent \textbf{Acknowledgments.} We thank the people involved in
the creation and maintenance of the Arcade Learning Environment that
have made this work possible. The research by Nir Lipovetzky is partially
funded by the Australian Research Council Linkage grant LP11010015.
}

%%% - Learning in same instance, across diff instances of same game, etc.
%%%% - Learning controllers vs. on-line planning, etc

%%% - Learning in same instance, across diff instances of same game, etc.
%%%% - Learning controllers vs. on-line planning, etc

\bibliographystyle{named}
\bibliography{control}

\end{document}
