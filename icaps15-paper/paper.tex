 
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{url}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}


%Table stuff
\usepackage{xcolor,colortbl}
\definecolor{Gray}{gray}{0.85}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\newcolumntype{a}{>{\columncolor{Gray}}c}
\newcolumntype{b}{>{\columncolor{LightCyan}}c}

\newcommand{\mc}[2]{\multicolumn{#1}{b}{#2}}



\newcommand{\tuple}[1]{{\langle #1\rangle}}
\newcommand{\triple}[1]{{\langle #1\rangle}}
\newcommand{\pair}[1]{{\langle #1\rangle}}

\newcommand{\Omit}[1]{}

\newcommand{\propername}[1]{\textsc{#1}}
\newcommand{\eqdef}{\stackrel{\hbox{\tiny{def}}}{=}}
%\newcommand{\IW}{{\textit{IW}}}
%\newcommand{\SIW}{{\textit{SIW}}}
%\newcommand{\ID}{{\textit{ID}}}
%\newcommand{\BRFS}{{\textit{BrFS}}}
%\newcommand{\BFS}{{\textit{\BFS}}}
%\newcommand{\UCT}{{\textit{UCT}}}
\newcommand{\IW}{{\textsc{IW}}}
\newcommand{\SIW}{{\textsct{SIW}}}
\newcommand{\ID}{{\textsc{ID}}}
\newcommand{\BRFS}{{\textsc{BrFS}}}
\newcommand{\BFS}{{\textsc{2BFS}}}
\newcommand{\UCT}{{\textsc{UCT}}}

\newcommand{\numgames}{$35$}

\newcommand{\Assault}{\propername{Assault}}
\newcommand{\Asterix}{\propername{Asterix}}
\newcommand{\Asteroids}{\propername{Asteroids}}
\newcommand{\Atlantis}{\propername{Atlantis}}
\newcommand{\BankHeist}{\propername{Bank Heist}}
\newcommand{\BattleZone}{\propername{Battle Zone}}
\newcommand{\BeamRider}{\propername{Beam Rider}}
\newcommand{\Berzerk}{\propername{Berzerk}}
\newcommand{\Bowling}{\propername{Bowling}}
\newcommand{\Boxing}{\propername{Boxing}}
\newcommand{\Breakout}{\propername{Breakout}}
\newcommand{\Centipede}{\propername{Centipede}}
\newcommand{\ChopperCommand}{\propername{Chopper Command}}
\newcommand{\CrazyClimber}{\propername{Crazy Climber}}
\newcommand{\Enduro}{\propername{Enduro}}
\newcommand{\Freeway}{\propername{Freeway}}
\newcommand{\Frostbite}{\propername{Frostbite}}
\newcommand{\Gravitar}{\propername{Gravitar}}
\newcommand{\Hero}{\propername{Hero}}
\newcommand{\JamesBond}{\propername{James Bond}}
\newcommand{\Kangaroo}{\propername{Kangaroo}}
\newcommand{\Krull}{\propername{Krull}}
\newcommand{\Montezuma}{\propername{Montezuma Revenge}}
\newcommand{\MsPacman}{\propername{Ms Pacman}}
\newcommand{\Pong}{\propername{Pong}}
\newcommand{\Pooyan}{\propername{Pooyan}}
\newcommand{\Qbert}{\propername{Q*Bert}}
\newcommand{\RiverRaid}{\propername{Riverraid}}
\newcommand{\Seaquest}{\propername{Seaquest}}
\newcommand{\SpaceInvaders}{\propername{Space Invaders}}
\newcommand{\StarGunner}{\propername{Star Gunner}}
\newcommand{\Tennis}{\propername{Tennis}}
\newcommand{\Venture}{\propername{Venture}}
\newcommand{\VideoPinball}{\propername{Video Pinball}}
\newcommand{\Zaxxon}{\propername{Zaxxon}}

\newcommand{\Deepmind}{\propername{Deepmind}}


% AAAI (comment for ECAI)
% \newtheorem{theorem}{Theorem}


 \newtheorem{theorem}{Definition}
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
 \newtheorem{definition}[theorem]{Definition}


 \begin{document}
% The file aaai.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%
\pdfinfo{
/Title (Classical Planning Algorithms on the Atari Video Games)
/Author ( )}
% (Nir Lipovetzky, Miquel Ramirez, Hector Geffner)}

\setcounter{secnumdepth}{0}

\title{Classical Planning Algorithms ¡ on the Atari Video Games}

\author{\textbf{ID:} 52 \ \ ; \ \ \textbf{Keywords:} classical planning, video games}

% \author{Nir Lipovetzky \\
%       The University of Melbourne \\
%    Melbourne, Australia\\
%    {\normalsize\url{@unimelb.edu.au}}
%    \And
%    Miquel Ramirez \\
%    Australian National University \\
%    Canberra, Australia\\
%   {\normalsize\url{@gmail.com}}
%    \And
%    Hector Geffner \\
%    ICREA \&  Universitat Pompeu Fabra \\
%    Barcelona, SPAIN \\
%    {\normalsize\url{@upf.edu}}\thanks{firstname.lastname}
% }


%AAAI Press\\
%Association for the Advancement of Artificial Intelligence\\
%2275 East Bayshore Road, Suite 160\\
% Palo Alto, California 94303\\}

\maketitle


%% Pending: randomization, discount factor, iw11 ..
%% iw2 multivalued expands up to 256^2 n^nodes, while iw2 boolean 16^2 n^2 nodes; can iw2 boolean beat iw1 multivalued?
%% Important. optimization, need Dikjstra por postprocessing .. see emails


\begin{abstract}
\begin{quote}
The Atari 2600 games supported in  the Arcade Learning Environment~\cite{bellemare:jair2013}
all feature a known initial (RAM) state and actions that have deterministic effects. Classical planners, however,
cannot be used  for selecting actions  for  two reasons: first,  no  compact PDDL-model of  the games is given,
and  more importantly,  the action effects and goals are  not known \emph{a priori}. Moreover, in these games there is usually
no set of goals to be achieved but rewards to be collected. These features do not preclude the use of   classical  algorithms like
breadth-first search or Dijkstra's algorithm,  but    these methods are not effective over  large state spaces. We thus turn to a
different class of classical planning algorithms  introduced recently  that perform a \emph{structured exploration} of the state space; namely,
like breadth-first search and Dijkstra's  algorithm they are ``blind'' and hence  do not require prior knowledge of state transitions,
costs (rewards) or goals, and yet, like  heuristic search algorithms, they have been shown to be   effective for solving   problems over huge
state spaces. The simplest such algorithm, called Iterated Width or \IW, consists of  a sequence of calls  $\IW(1)$, $\IW(2)$, \ldots,
$\IW(k)$ where  $\IW(i)$ is  a breadth-first search in which a state is  pruned when it is not the
first state in the search  to make true some subset of $i$ atoms. 
The  empirical results over 35 games suggest that the performance 
of \IW\ with the  $k$ parameter fixed to $1$, i.e., $\IW(1)$, is at the level of the state of the art represented by 
\UCT.  A simple best-first variation  of \IW\  that combines exploration and exploitation proves
to be very competitive as well. 
\end{quote}
\end{abstract}

\section{Introduction}


The Arcade Learning Environment (ALE)  provides a challenging  platform for evaluating general, domain-independent AI planners and
learners through a convenient interface to hundreds of Atari 2600 games \cite{bellemare:jair2013}.
Results have been reported so far for basic planning algorithms like breadth-first
search and  \UCT,  reinforcement learning algorithms, and evolutionary  methods \cite{bellemare:jair2013,deep-mind-atari,risto-texas:atari-games}.
The empirical results are impressive in some cases, yet a lot remains to be done, as no method  approaches   the
performance of  human players across a broad range of games.

While all  these games feature a known initial (RAM) state and actions that
have deterministic effects, the problem of selecting the next action
to be done cannot be addressed with state-of-the-art classical planners \cite{geffner:book}.
This is because there is  no compact PDDL-like encoding of the domain, and more
importantly,  the goal to be achieved in each game  is not given. Indeed, there are
often no goals  but rewards $r(a,s)$ that  depend on the action $a$ and
the state $s$ in  which  the action is performed, and these rewards are not known.\footnote{Actually, in the Atari games, the rewards $r(a,s)$ depend only
on the state $f(a,s)$ that results from doing action $a$ in the state $s$.}
Thus no model of the  goals  or the  rewards can be used to bias the search.

The action selection problem in the Atari games can be seen as a \emph{reinforcement learning} problem
\cite{sutton:book} over a deterministic MDP where the state transitions and rewards are not known, or alternatively,
as a  \emph{net-benefit planning problem} \cite{coles2012survey,keyder:jair09} with unknown state transitions  and  rewards. 
Namely, we seek  an action sequence $a_0, a_1, \ldots, a_m$ from the  current  state $s_0$ that generates
a state sequence $s_0, s_1, \ldots, s_{m+1}$ with  maximum  total reward $\sum_{i=0}^{m} r(a_i,s_i)$,
where $m$ is a sufficiently large planning horizon, and the rewards $r(a_i,s_i)$ and the  state-transitions $s_{i+1}=f(a_i,s_i)$
are known only after  action $a_i$ is applied in the state $s_i$.

The presence of unknown transition and  rewards in  the Atari games does not preclude the use of blind-search
methods like breadth-first search, Dijkstra's algorithm \cite{dijkstra},  or learning methods  such as
LRTA* \cite{korf:lrta}, \UCT\ \cite{uct}, and Q-learning  \cite{sutton:book,bertsekas:neuro}. Indeed,
the net-benefit planning problem with unknown state transitions and rewards over a given planning
horizon, can be mapped into a standard \emph{shortest-path problem} which can be solved optimally by Dijkstra's algorithm.
For this, we just need to  map the unknown rewards $r(a,s)$ into positive (unknown) action costs $c(a,s) = C - r(a,s)$
where  $C$ is a large constant that exceeds the maximum possible reward. The fact that the state
transition and cost functions  $f(a,s)$ and $c(a,s)$ are not known a priori doesn't affect the applicability of
Dijkstra's algorithm, which requires the value of these functions precisely when  the action $a$
is applied in the state $s$.

The limitation of the   basic blind search  methods is that they are not effective  over large  spaces, neither
for solving problems off-line, nor for guiding  a lookahead search for solving problems on-line.
In this work, we thus turn to a recent class of  planning algorithms that combine the scope of blind search
methods with the performance of state-of-the-art classical planners: namely, like ``blind'' search algorithms
they do not require prior knowledge of state transitions, costs, or goals, and yet like heuristic algorithms
they manage to search large  state spaces effectively. The basic algorithm in this class is
called \IW\ for Iterated Width search \cite{nir:ecai2012}. \IW\ consists of a sequence of calls  $\IW(1)$, $\IW(2)$, .., $\IW(k)$, where
$\IW(i)$ is  a  standard breadth-first search where states are pruned right away when they fail to make true some new tuple (set)
of at most $i$ atoms. Namely,   $\IW(1)$ is a breadth-first search that keeps  a state only if  the state is   the first  one
in the search to make some atom  true; $\IW(2)$ keeps a state  only if the state is  the first one to  make  a pair of atoms true, and
so on.  Like plain breadth-first  and iterative deepening searches,  \IW\  is
complete, while searching the state space in a way that makes use of the
\emph{structure of states} given by the values of a finite set of state variables.
In the Atari games, the (RAM) state is given by a vector of 128 bytes, which we associate with 128 variables
$X_i$, $i=1, \ldots, 128$, each of which may take up to $256$ values $x_j$. A state $s$ makes an atom $X_i=x_{j}$
true when the value of the $i$-th byte in the state vector $s$ is $x_j$.

While a normal, complete breadth-first search runs in time that is exponential in the total number of variables $X_i$,
the procedure $\IW(k)$ runs in time that is exponential in the $k$ parameter, $1 \leq k \leq n$.
Elsewhere, it has been shown that  \IW\ exhibits   state-of-the-art performance on most existing planning benchmark domains
when goals are restricted to single atoms  where \IW\ provably runs in low polynomial time in the number of
variables \cite{nir:ecai2012}. For example, in a blocks world instance with any number of blocks and any initial
configuration,  the goal of  placing a given  block on top of another will be achievable in  quadratic time
in the number of blocks as any such instance can be shown to  have \emph{width} 2. This means  that $\IW(2)$ will
not only be complete then, but also optimal. This is also true for most other benchmark domains in classical planning
that turn out to  have small, bounded widths for \emph{any} instance as long as the goal is atomic.
For dealing with the benchmarks where goals are  not single atoms, e.g., where a given \emph{tower} of blocks needs to be built,
simple extensions of the basic \IW\ procedure have been developed  such as Serialized \IW, where \IW\ is
used to achieve the goals one at at time following a simple goal ordering  \cite{nir:ecai2012,nir:ecai2014}.
The extension of \IW\ for problems where there is no goal but an
additive reward measure  to be optimized, like in the Atari games, is direct.

%%  Each iteration IW(k) keeps track of the accumulated reward up to each state $s$ that has been
%% generated but  not pruned, as $R(s) = R(s')+ r(a,s')$ where $s'$ and $a$ are  the single parent state
%% and action $a$ that led to $s$ in IW(k) with $R(s)=0$ for the current state $s$. The best reward

% \Omit{
% The sequence of calls IW(1), IW(2), \ldots, IW(k) manages to combine the scope of ``blind'' search algorithms with the performance of
% heuristic-search algorithms by exploring states  in an  order dictated by the ``width''  parameter $k$;
% states that have width 1 are explored first in linear time, then states that have  width two are explored in quadratic time, and so on.
% In classical planning problems, if the goal has a small width (something common in the classical planning domains when the  goal is a single atom, as
% we have seen above), IW will find goal states  in low polynomial time. Similarly, in the reward-based setting, if  there are states of high reward
% that have small width, IW will find such states in low polynomial time, even if the state space is exponential in the number of variables.
% In this paper, we aim to test whether these ideas work for the Atari games too  when  the RAM state of the emulator
% is used  as the system state.
% }

The paper is organized as follows. We review the iterated width  algorithm and where it comes from,
look at  simple   variations of the algorithm that appear to be convenient  for the Atari
games, present and analyze the  experimental results, and discuss related and future work.

\section{Iterated Width}

The Iterated Width  (\IW) algorithm has been introduced as a classical planning algorithm
that takes a planning problem as an input, and computes an action sequence that solves the problem
as the output \cite{nir:ecai2012}. The algorithm however
applies to a broader range of problems. We will characterize such
problems  by means of  a finite and discrete set of states (the state space)
that correspond to vectors of size $n$. Namely, the
states are  \emph{structured} or \emph{factored} , and we take each of the locations in the vector
to represent a variable $X_i$, and the value at that vector location to
represent the value $x_j$ of variable $X_i$ in the state.
In addition to the state space, a problem is defined by an initial state $s_0$,
a set of actions applicable in each state, a
transition function $f$ such that  $s' = f(a,s)$ is
the state that results from applying action $a$  to the state $s$,
and rewards $r(a,s)$ represented by real numbers that result from  applying action $a$ in state $s$.
The transition and reward functions do not need to be known \emph{a priori}, yet in that case, the
state and  reward that results from the application of an action in a state needs to be \emph{observable}.
The task is to compute an action sequence $a_0, \ldots, a_m$ for a large horizon $m$
that generates a state sequence $s_0, \ldots, s_{m+1}$ that  maximizes the accumulated
reward $\sum_{i=0}^{m} r(a_i,s_i)$, or that provides a good approximation.

\subsection{The Algorithm}

\IW \ consists of a sequence of calls  $\IW(i)$ for $i=0, 1, 2, \ldots$
over a problem $P$ until a termination condition is reached.
The procedure $\IW(i)$  is a plain forward-state \emph{breadth-first search} with just one change: right after a state
$s$ is generated, the state is  pruned if it doesn't pass a simple \emph{novelty test}.
More precisely,

\begin{itemize}
\item The \emph{novelty of a newly generate state $s$  in a  search algorithm}
is   $1$ if  $s$ is the first state generated in the search that makes true some  atom $X=x$,
else it is $2$ if  $s$ is the first state that makes a \emph{pair} of atoms $X=x$ and $Y=y$ true,
and so on.

\item  $\IW(i)$ is a breadth-first search that prunes newly generated states when their novelty measure
is greater than $i$.

\item $\IW$  calls $\IW(i)$ sequentially for $i=1,2,\ldots$ until a termination condition is reached,
returning then the best path found.
\end{itemize}

For classical planning, the termination condition is the achievement of the goal. In the on-line setting,
as in the Atari games, the termination condition is given by a time window or a maximum number of generated nodes.
The  best path found by \IW \ is the path  that has a  maximum accumulated reward.
The accumulated reward $R(s)$  of a state $s$ reached in an iteration of \IW \ is determined by the  unique parent state $s'$
and action $a$ leading to $s$ from $s'$ as $R(s) = R(s') + r(a,s')$. The best state is the state $s$ with maximum reward
$R(s)$ generated but not pruned by \IW, and the best path is the one that leads to the state $s$ from the current state.
The action selected to be done next is  the first action along  such a path.

%% Like in standard breadth-first search, each iteration IW(i)  prunes
%%% the  duplicate states as well.

\begin{table}[t!]
\begin{center}
%\resizebox{3in}{!}{
{\scriptsize
% \begin{tabular}{r@{}lrrrr@{}}
\begin{tabular}{llcccc}
%% table header
\hline\\[-1.3ex]
%\toprule
\# & \multicolumn{1}{c}{Domain} & \multicolumn{1}{c}{I} & \multicolumn{1}{c}{$\IW(1)$} & \multicolumn{1}{c}{$\IW(2)$} & \multicolumn{1}{c}{Neither} \\[.2ex]

\hline\\[-1.3ex]
%\hline
%% table body
1. & 8puzzle	&400	&55\%	&45\%	&0\%\\
2. & Barman	&232	&9\%	&0\%	&91\%\\
3. & Blocks World	&598	&26\%	&74\%	&0\%\\
4. &Cybersecure	&86	&65\%	&0\%	&35\%\\
% 5. &Depots	&189	&11\%	&66\%	&23\%\\
% 6. & Driver	&259	&45\%	&55\%	&0\%\\
% 7. & Elevators	&510	&0\%	&100\%	&0\%\\
% 8. & Ferry	&650	&36\%	&64\%	&0\%\\
% 9. & Floortile	&538	&96\%	&4\%	&0\%\\
% 10. & Freecell	&76	&8\%	&92\%	&0\%\\
% 11. & Grid	&19	&5\%	&84\%	&11\%\\
% 12. & Gripper	&1275	&0\%	&100\%	&0\%\\
% 13. & Logistics	&249	&18\%	&82\%	&0\%\\
% 14. & Miconic	&650	&0\%	&100\%	&0\%\\
% 15. & Mprime	&43	&5\%	&95\%	&0\%\\
% 16. & Mystery	&30	&7\%	&93\%	&0\%\\
% 17. & NoMystery	&210	&0\%	&100\%	&0\%\\
% 18. & OpenStacks	&630	&0\%	&0\%	&100\%\\
% 19. & OpenStacksIPC6	&1230	&5\%	&16\%	&79\%\\
\ldots & \ldots & \ldots & \ldots & \ldots & \\
20. & ParcPrinter	&975	&85\%	&15\%	&0\%\\
21. & Parking	&540	&77\%	&23\%	&0\%\\
22. & Pegsol	&964	&92\%	&8\%	&0\%\\
23. & Pipes-NonTan	&259	&44\%	&56\%	&0\%\\
24. & Pipes-Tan	&369	&59\%	&37\%	&3\%\\
25. & PSRsmall	&316	&92\%	&0\%	&8\%\\
26. & Rovers	&488	&47\%	&53\%	&0\%\\
27. & Satellite	&308	&11\%	&89\%	&0\%\\
28. & Scanalyzer	&624	&100\%	&0\%	&0\%\\
29. & Sokoban	&153	&37\%	&36\%	&27\%\\
30. & Storage	&240	&100\%	&0\%	&0\%\\
31. & Tidybot	&84	&12\%	&39\%	&49\%\\
32. & Tpp	&315	&0\%	&92\%	&8\%\\
33. & Transport	&330	&0\%	&100\%	&0\%\\
34. & Trucks	&345	&0\%	&100\%	&0\%\\
35. & Visitall	&21859	&100\%	&0\%	&0\%\\
36. & Woodworking	&1659	&100\%	&0\%	&0\%\\
37. & Zeno	&219	&21\%	&79\%	&0\%\\[.2ex]
%\hline
\hline\\[-1.3ex]
& Total/Avgs&37921	&37.0\%	&51.3\%	&11.7\%\\
\hline
\end{tabular}
}

\vskip .1cm


{\scriptsize
\begin{tabular}{@{}ccccc@{}}
\hline\\[-1.3ex]
\multicolumn{1}{c}{\# Instances}  &\multicolumn{1}{c}{\IW} & \multicolumn{1}{c}{\ID} & \multicolumn{1}{c}{\BRFS} &\multicolumn{1}{c}{\textit{GBFS +} $h_{add}$} \\[.2ex]
\hline\\[-1.3ex]
37921  &34627 &9010	&8762 &34849 \\
\hline
%\hline
\end{tabular}
}

\end{center}

\caption{\small
\emph{Top:} Number of classical planning instances per domain  and percentages solved by $\IW(1)$, $\IW(2)$, or neither.
Problems  obtained by splitting benchmarks  with $N$ atomic goals into $N$ problems with atomic goals.
\emph{Bottom:} Number of instances solved by \IW\ in comparison with  Iterative Deepening (\ID),
Breadth-First Search (\BRFS),   and Greedy Best First Search (\textit{GBFS}) guided by  additive heuristic.
Time and  memory outs  after 30 minutes or 2 GB. Table from \cite{nir:ecai2012}.
}
\label{table1}
\end{table}
%%%%
%%%

\subsection{Performance and Width}

\IW\ is a systematic and complete blind-search algorithm like breadth-first search (\BRFS) and iterative deepening (\ID),
but   unlike these algorithms, it uses the factored representation of the states in terms of variables to structure the
search in a different way. This structured exploration  has proved to be very
effective over classical planning benchmark domains  when goals are  single atoms.\footnote{
Any conjunctive goal can be mapped into a single dummy atomic goal by adding an action that
achieves the dummy goal and that has the original conjunctive goal as a precondition.
Yet, this changes the definition of the domain.}
Table~\ref{table1} from \cite{nir:ecai2012} shows
the percentage of instances that are  solved by  the first iteration of \IW, i.e.  $\IW(1)$, by the second iteration
$\IW(2)$, and by neither one.  These are instances that  have been obtained from the existing benchmarks
by splitting problems with $N$ atomic goals, into $N$ problems with one atomic goal each. As the table shows,
$37\%$ of the 37921 instances are solved by $\IW(1)$ while $51.3\%$ are  solved by $\IW(2)$. Since $\IW(k)$
runs in time that is exponential in $k$, this mean that almost $90\%$ of the 37,921 instances are  solved in time that
is either linear or quadratic in the number of problem variables, which in these encoding are all \emph{boolean}.
Furthermore, when the performance of \IW\   is compared with  breadth-first search and iterative deepening,
on the one hand,  and  with a  Greedy Best First Search guided by the additive heuristic $h_{add}$
\cite{bonet:aij-hsp} on the other (this algorithm is similar
to the one used by  the FF planner when hill climbing search fails \cite{hoffmann:ff}),
it turns out that ``blind'' \IW\ solves as many problems as the informed search, 34,627 vs. 34,849, far ahead
of the other blind algorithms \BRFS\ and \ID\  that solve 9,010 and 8,762 problems each. This is shown in the bottom
part of  Table~\ref{table1}. Moreover, \IW\ is  faster and results in shorter plans than a heuristic search algorithm
\cite{nir:ecai2012}.

The min $k$ value for which $\IW(k)$ solves a problem  is bounded and small in most of
these instances. This is  actually no  accident and  has a \emph{theoretical explanation.} Lipovetzky and Geffner define
a structural parameter called the problem \emph{width} and show that for many of these domains,
\emph{any} solvable instance with atomic goals will  have a bounded and small width that is independent of the number
of variables and states in the problem. The min value $k$ for which the iteration  $\IW(k)$ solves
the problem cannot exceed the problem width, so  the algorithm \emph{\IW\ runs in time and space
that are exponential in the problem width.}

Formally, the \emph{width} $w(P)$ of a problem $P$ is $i$
iff $i$ is the minimum positive integer for which
there is a sequence of tuples (sets) with at most $i$ atoms each, $t_0, t_1, \ldots, t_n$
such that 1)~$t_0$ is true in the initial state of $P$, 2)~any shortest plan $\pi$  that achieves $t_i$ in $P$
can be extended into a shortest plan that achieves $t_{i+1}$ by extending   $\pi$ with one action, and
3)~any shortest plan that achieves $t_n$ is also a shortest plan for $P$.

One way to understand this definition is that the problem  width $w(P)$ is at most  $i$ if  there is a
``trail of  stepping stones''  $t_0, t_1, \ldots, t_n$ to reach the problem goal such that
A)~these ``stepping stones''  contain at most $i$ atoms each, B)~each  stepping stone $t_{i+1}$ is
at distance 1 from the previous one $t_i$ when $t_i$ has been reached following the ``trail'',
C)~the ``trail''  preserves ``optimality''; i.e., no  tuple $t_i$ can be reached in less than $i$ steps.

While this notion of width and the iterated  width algorithms that are based on it  have been designed
for problems where a goal state needs to be reached, the notions remain
relevant  in  optimization problems as well. Indeed, if a  good path is made of
states $s_i$ each of which has a low width, \IW\ can be made
to find such path in low polynomial time for a small value of the $k$ parameter.
Later on we will discuss  a slight  change  required in \IW\ to enforce this property.
% which is actually  not yet implemented in the code that is tested below.


\section{The Algorithms for the Atari Games}

The number of nodes \emph{generated} by $\IW(1)$ is $n \times D \times b$ in the worst case,
where $n$ is the number of variables, $D$ is the size of their domains, and $b$ is the number of
actions per  state.  This same number in breadth-first search is not linear in $n$
but exponential. For the Atari games,  $n=128$, $D=256$, and $b=18$, so that the product is
equal to $589,824$, which is large but feasible. On the other hand, the number
of nodes generated by $\IW(2)$ in the worst case is  $(n \times D)^2 \times b$, which is
equal to $1,073,741,824$ which is too large, forcing us to consider only a tiny fraction of
such states. For classical planning problems, the growth in the number of nodes from
$\IW(1)$ to $\IW(2)$ is not that large, as the variables are boolean. Indeed, we could have
taken the state vector for the Atari games as a vector of 1024 boolean variables,
and apply these algorithms to that representation. The results would be different.
In such a case, $\IW(1)$ and $\IW(2)$  would generate $n' \times D' \times b$ and
$(n' \times D')^2 \times b$ states, which for $n'=1024$ and $D=2$
represent  $36,864$ and $7,5497,472$ states respectively.
These are feasible numbers, but we haven't used  this representation 
under the assumption that the correlations among the bits in each one of
the 128 words of the state vectors are meaningful.
In summary, from the basic \IW\ algorithm we are testing only the first
call $\IW(1)$.


\IW\ is a purely exploration algorithm that does not take into account  the accumulated
reward for selecting the states to consider. As a simple variant that combines
exploration and exploitation, we evaluated a \emph{best-first search} algorithm
with two queues: one queue  ordered first by  novelty measure
(recall that novelty one means that the state is the first one to make some atom true),
and a second queue ordered by accumulated reward. In one iteration, the best first
search picks up the best node from one queue, and in the second iteration it picks
up the best node from the other queue. This way for combining multiple heuristics
is used in the LAMA planner \cite{richter:lama}, and was introduced in the
planner Fast Downward \cite{helmert:fd}. In addition, we break ties
in the first queue favoring states with largest accumulated reward, and
in the second queue, favoring states with smallest novelty measure. Last, when
a node is expanded, it is removed from the queue, and its children are placed on
both queues. The exception are the nodes with no accumulated reward  that are
placed in the first queue only. We refer to this best-first algorithm
as \BFS.

For  the experiments below, we added two simple variations to  $\IW(1)$ and \BFS.
First, in the breadth-first search underlying $\IW(1)$, we generate the children in
random order. This makes the executions that results from the
$\IW(1)$ lookahead less susceptible to fall into loops; a potential
problem in local search algorithms with no memory or learning.
Second, a discount factor $\gamma=0.995$ is used in both algorithms
for discounting future rewards like in \UCT.
For this, each state $s$ keeps its depth $d(s)$ in the search tree,
and if state s' is the child of state $s$ and action $a$,
$R(s')$ is set to $R(s) + \gamma^{d(s)+1} r(a,s)$.
The  discount factor  results in a slight
preference for rewards that can be reached earlier,
which is a  reasonable heuristic in  on-line settings
based on lookahead searches.

\section{Experimental Results}

\input{games-table}
We tested \IW(1) and \BFS\ over \numgames\
 of the $55$ games considered in
\cite{bellemare:jair2013}, from now on  abbreviated as BNVB.\footnote{
Entries marked as  ``--'' in the table are   still running at the time of submission. 
Each run can take up to $24$ hours of CPU time and there are $10$ runs per game and algorithm. 
Eventually all of  the  $55$ games will  be included in the evaluation.}
Table~\ref{performance} shows the performance of $\IW(1)$ and \BFS\
% , characterized by the average score obtained over all episodes, the standard deviation
% of the score obtained and time (in seconds) per lookahead,
in comparison  with breadth-first search (\BRFS) and \UCT.
Videos of selected games played by $\IW(1)$, \BFS, and \UCT\ can be 
seen in Youtube.\footnote{\url{http://www.youtube.com/playlist?list=PLXpQcXUQ_CwenUazUivhXyYvjuS6KQOI0}.}
The discount factor used by  all the algorithms is $\gamma=0.995$.
The scores reported for \BRFS\ and \UCT\ are taken from BNVB.  
Our experimental setup  follows theirs except that a maximum budget of  $150,000$ simulated frames 
is applied to \textit{all the algorithms}. \UCT\ uses this budget by running
 $500$   rollouts of depth $300$. % \footnote{In \UCT,  $500$ base policy rollouts
% of depth $300$ is equivalent to a budget of $150,000$ simulated frames.}.
The bound on the number of simulated frames is like a bound on
lookahead time, as most of the time in the lookahead  is spent
in calls to the emulator for computing the next RAM state. 
This is why the average time per action is similar to all the algorithms
except $\IW(1)$, that due to its aggressive pruning does not always use
the full budget and takes less time per action on average.

Also,  as reported by  BNVB, all of the algorithms 
reuse the sub-tree from the previous lookahead that is rooted in the selected 
child, deleting its siblings and their descendants. More precisely, no calls 
to the emulator are done for transitions that are cached in that sub-tree, 
and such reused frames are not discounted from the budget that is thus a bound 
on the number of ``new'' frames per lookahead. In addition, in $\IW(1)$, the states that 
are ``reused'' from the previous searches are ignored in the computation of the novelty of 
new states so that more states can escape pruning. Otherwise, $\IW(1)$ often
uses a fraction of the  budget.  This is not needed in \BFS\ which does 
no pruning.  $\IW(1)$ and \BFS\ are  limited to  search up to a depth of
$m=1,500$ frames and up to $m=150,000$ frames per root branch. This is
to avoid the search from  going too deep or being too committed to a
single root action. 
% Similarly UCT defines a limit of $m=300$ frames for each rollout. 
% Note that UCT is more likely to have a performance
% loss if the depth is increased, as the number of rollouts would
% decrease in order to maintain the same budget of $150,000$ simulated
% frames. 

Last, \IW(1) and \BFS\ select an action every $5$ frames, while \UCT\  selects 
an action in every frame.  This means that in order to explore a branch 
$300$ frames deep, UCT gets to choose $300$ actions, while \IW($1$) and \BFS\ 
get to choose $60$ actions, both however using the same $300$ frames from the
budget. For this, we  followed the setup  of \BRFS\ in BNVB 
that also selects actions every $5$ frames, matching the behavior
of the emulator that requests an action also every $5$ frames. Since the 
lookahead budget is given by a maximum number of (new) frames, and the time 
is mostly taken by calls to the emulator, this may not be the best choice  
for $\IW(1)$ and \BFS\ that may therefore not be exploiting all the 
options afforded by the budget. We hope to investigate the implications of
this gap in the future.

\Omit{
The last difference inherited from BNVB between all algorithms
and \UCT\, is that \UCT\ selects an action randomly in the rollout every
single frame, while the others select an action every 5 frames, and
the remain ones are $no-op$, as is done in the simulation
environment. Therefore, a node in all algorithms represent $5$ frames,
but in UCT represents $1$ frame. As a result, a UCT rollout of depth
$300$ frames involve the selection of 300 actions, decreasing the
budget of simulated frames by $300$, while the other algorithms need a
branch of $1500$ frames to select $300$ actions, decreasing the budget
by $1500$ frames instead.
}


Table~\ref{performance} shows that both $\IW(1)$ and \BFS\ generally
 outperform \BRFS, which rarely collects any reward in many domains, as 
 the depth of the \BRFS\ search tree results in a lookahead of $0.3$
seconds ($20$ frames or $4$ nodes deep). The notable exception to this
is \Centipede\ where abundant reward can be collected with a shallow
lookahead. On the other hand, both $\IW(1)$ and \BFS\ reach states that 
are up to $350$--$1,500$ frames deep ($70$--$260$ nodes or $6$--$22$ seconds), 
even if $\IW(1)$ does not always use all the simulation frames allocated due to its
pruning. This behaviour can be observed in games such as
\Breakout, \CrazyClimber, \Kangaroo, and \Pooyan, where the average
CPU time for each lookahead is up to $10$ times faster than \BFS. 
Computation time for \UCT\ and \BRFS\ are  similar to \BFS, as the
most expensive part of the computation is the  generation of 
frames through the simulator,  and these three  algorithms 
always use  the full budget.


More interestingly,  \IW(1) outscores \UCT\  in $17$ of the \numgames\ games,
while \BFS\ outscores \UCT\ in $18$.
%\footnote{These numbers can actually
%improve further as 6  games for \IW(1) and 4 games for \BFS\ are still running.}
On the other, \UCT\ does better than $\IW(1)$ and \BFS\ in $9$ and $11$ games respectively.
The relative performance between $\IW(1)$ and \BFS\ is closer with $\IW(1)$ being  the
best of the two in $12$ games, and  \BFS\  in $11$. In terms of the number of games 
where an algorithm is the overall winner, the situation is  even: 
$\IW(1)$ is the overall best in $12$ games, \BFS\ in $11$ games, and \UCT\ in $12$ games. 
Also, \BRFS\ is best in $1$ game (\Centipede), while the other three algorithms
are tied in another game (\Pong).

\Omit{As mentioned in the
previous section, the order in which $\IW(1)$ and \BFS\ generate the
children of an expanded node is randomized, something which has a
potential effect on the novelty measures of nodes, and hence in the
search graph. We found that this randomization helps, in particular in
\Qbert, where not randomizing results in $\IW(1)$ being trapped into
loops sometimes.}


Likewise, in \Freeway\ and \Berzerk\ both $\IW(1)$ and \BFS\ attain a better
score than the baseline semi-random algorithm \emph{Perturb} in
\cite{bellemare:jair2013}, that beats \UCT\ on those games.
%% to say the 2 lines below must add context: .. eg
%%% ``which in *these games* outperforms both \UCT\ and the reinforcement learning methods.''
%% add it if that's the case ..
\emph{Perturb} is a simple algorithm
that selects  a fixed  action with probability $0.95$, and a random
action with probability $0.05$.  For \emph{Perturb}, BNVB do not report 
the average score  but the  best score.  \emph{Perturb} manages to do well in domains where rewards
are deep but can be reached by  repeating the same action.
This is the case of \Freeway, where a chicken has to run across a ten lane
highway filled with traffic. Every time the chicken gets across, there is one
unit of reward. If the chicken is hit by a car, it has to start again. 
In \Freeway, only $12$ out of $18$ possible actions have an effect:
$6$ actions in moving the chicken up one lane (up-right, up-left, up-fire, up-right-fire, up-left-fire), 
$6$ in moving the chicken down one lane (down-right, down-left, down-fire, down-right-fire, down-left-fire), 
and $6$ actions do nothing.  \emph{Perturb}  does well in this domain 
when the selected fixed action is one of the $6$ that moves the chicken up.  As noted 
in Table~\ref{performance} and seen in the provided video, \UCT\ does not manage to take the chicken
across the highway at all. The reason for \UCT\ to perform erratically 
is that the roll-outs do not manage to collect any reward. A sequence of actions 
with at least $10$ actions embedded that move the chicken up are required in order 
to collect a reward. Since the probability for the roll-out to choose an action that moves the chicken up 
is $1/3$, with the parameters chosen by BNVB, the likelihood of \UCT\ generating such a sequence of actions 
that \emph{also} avoids incoming traffic in a timely fashion is rather low.
%% the previous number was not correct; no-op actions do not count, and thus needs
%%% 200 more move up actions than move down actions, although this is not accurate either
%%%% as move down actions that take chicken below the screen count as no-ops too ..
$\IW(1)$ does not have this limitation and is best in \Freeway.

%%
%%% Sobre texto below; no me convence; no sabemos como cambia el RAM aunque podamos suponer
%%% obvio: si un byte codifica donde esta el chicken en la pantalla .. tendriamos una explicación
%% On the other hand, \IW\ manages to find rewards without exhausting the  budget. The main reason that
%% explains this behaviour is that at most only 3 generated states from
%% the same parent node will pass the novelty test, as in any expanded
%% node there is at most 3 different new possible bytes in the RAM state
%% resulting from moving up, down, or do-nothing.

$\IW(1)$ also outperforms the best learning algorithm in
\cite{deep-mind-atari} in the $7$ games considered there, and
\BFS\ does so in $6$ of the $7$ games. Comparing 
with the scores reported for the best Reinforcement Learning algorithm
in \cite{bellemare:jair2013}, we note that either $\IW(1)$ and \BFS\ 
are outperforming significantly the best learning algorithm, in those games
where it is reported to perform better than \UCT\ (\Montezuma, \Venture\
and \Bowling). In \Montezuma\ rewards are very
sparse, deep, and most of the actions lead to losing a life with
no immediate penalty or consequence.  All algorithms achieve $0$ score, 
except for \BFS\ that achieves an average score of $80$, and a score
of $400$ in one of the runs. This means that even \BFS\  is not able to 
consistently find rewards in this game. This game and several other games 
like \Breakout, and \SpaceInvaders\ could be much simpler by adding
negative rewards for losing a life. We have indeed observed that
our planning algorithms do not care much about losing lives
until there is just one life left, when their play noticeably improves.
This can be seen in the videos mentioned above, and suggests that
 some simple form of learning would be useful both
for planners and reinforcement learning approaches.

We are not  reporting  the performance of $\IW(k)$ with parameter $k=2$ because  in our
preliminary tests and according to the discussion in the previous section,
it doesn't  appear to improve much on  \BRFS, even if  it results in a lookahead
that is $5$ times deeper,  but  still too shallow  to compete with the other
planning algorithms.

\section{Exploration and Exploitation}

The notion of width underlying the iterated width algorithm was developed in the context
of classical planning in order to understand why most of the hundreds of existing benchmarks
appear to be relatively simple for current planners, even
though classical planning is PSPACE-complete \cite{bylander:complexity}. A  partial answer is that
most of these domains have a low width, and hence, can be solved in low polynomial time (by \IW)
when goals contain a single atom.  Benchmark problems with multiple atomic goals tend to be
easy too, as the goals can often be achieved one at a time after a simple goal ordering \cite{nir:ecai2012}.

In the iterated width algorithm, the key notion is the \emph{novelty measure} of a state in the underlying
breadth-first search. These novelty measures make use of the factored representation of the states for
providing a structure to the search: states that have width 1 are explored first in linear time,
then states that have  width 2  are explored in quadratic time, and so on. In classical planning problems
with atomic goals, this way of organizing the search  pays off both theoretically and practically.

The use of ``novelty measures'' for guiding an optimization search while ignoring the function that is being
optimized is   common to the  novelty-based search methods developed independently
in the context of genetic algorithms \cite{lehman:novelty}. %%% bibtex for this below
In these methods individuals in the population are not ranked according to the  optimization function
but in terms of how ``novel'' they are in relation to the rest of the population, thus encouraging diversity
and exploration rather than (greedy) exploitation. The actual definition of novelty
in such a case is domain-dependent; for example, in the evolution of a controller
for guiding a robot in a maze, an individual controller  will not be ranked by how
close it takes the robot to the goal (the greedy measure), but by the distance between
the locations that are  reachable with it, and the locations reachable with
the  other controllers in the population (a diversity measure). The novelty measure used by \IW, on the other hand, is domain-independent and it
is determined by the structure of the states as captured by the problem variables.






The balance between exploration and exploitation has received considerable attention in
reinforcement learning \cite{sutton:book}, where both are required for converging to an
optimal behavior. In the Atari games, as in other deterministic problems, however, ``exploration''
is not needed for optimality purposes, but just for improving the effectiveness of the
 lookahead search. Indeed, a best-first search algorithm guided only  by (discounted)
accumulated reward will deliver eventually  best moves, but it will not be as effective
over small time windows, where like breadth-first search it's likely not to find any rewards
at all. The \UCT\ algorithm provides a method for balancing exploration and exploitation,
which  is  effective over small time windows. The \BFS\ algorithm above with two queues
that alternate, one guided by the novelty measures  and the other by the accumulated reward,
provides a different scheme. The first converges to the optimal behavior asymptotically; the
second in a bounded number of steps, with the caveat below.

% Alternatively, novelty-guided probes, possibly with
% some noise added, could potentially pay off as  a  base policy for \UCT\ in replacement
% of random roll outs, as they apparently manage to exploit the structure of many problems.


\section{Duplicates and Optimality}

The notions of width and the \IW\  algorithm guarantee that  states with low width will be generated
in low polynomial time through \emph{shortest} paths. In the presence of rewards like the Atari games,
however, the  interest is not  in the shortest paths but in the \emph{best} paths; i.e, the paths with
maximum reward.  \IW\ may actually  fail to find such paths even when calling \IW($k$) with a high 
$k$ parameter. Optimality  could be  achieved then  by replacing the breadth-first search
underlying \IW($k$) by Dijkstra's algorithm yet such a move would make the relation between
\IW\ and the notion of width less clear. A better  option is to change  \IW\ to comply with 
a different property; namely, that if there is a ``rewarding'' path made up of states of 
low width, then \IW\ will find such paths or better ones in time that is exponential 
in their width. For this,  a simple change in \IW\ suffices: when generating a state 
$s$ that is a \emph{duplicate} of a state $s'$ that has been previously generated and not pruned, 
$s'$ is replaced by $s$ if $R(s) > R(s')$. Moreover in such a case, the change in reward 
must be propagated to the descendants of $R(s')$ that are in memory. This is similar to the change required 
in the  A* search algorithm for preserving optimality when moving from consistent to inconsistent heuristics \cite{pearl:heuristics}. 
The alternative is to ``reopen'' such nodes. The same change is actually needed in \BFS\ to ensure that, 
if given enough time, \BFS\ will actually find optimal paths.
% Changing \BFS\ to account for such paths guarantees that, if given enough time, \BFS\ will actually find
% the optimal paths. 
The code used for \IW\ and \BFS\ in the
experiments above does not implement this change  as the overhead involved in checking for duplicates 
in some test cases did not appear to pay off. More experiments  however are  needed to
find out if this is actually the most effective option.

\section{Summary and Future Work}

We have shown experimentally that width-based algorithms like $\IW(1)$ and \BFS\
that originate in work in classical planning,  can be used to play  the Atari video
games where they  achieve  state-of-the-art performance. This level of play 
is the result of a structured exploration of the state space that manages to combine 
the scope of blind search algorithms with the efficiency of heuristic search methods. 
There are several  issues worth studying, we  mention two of them.
Regarding planning, the results show the potential of width-based algorithms
for a broader class of planning problems. As shown, indeed, these algorithms
do not require PDDL encodings and can deal with state successor functions
encoded \emph{procedurally} (as in simulators) as long as the structure of the states 
is known. This suggests to us that the algorithms discussed in the 
present paper could be applied in a broad variety of settings where only on-line 
action selection is feasible or suitable.

Regarding the Atari games, we identify two problems in all the planning methods
discussed. First, by using the RAM state of the Atari console one can consider
that they are ``cheating'', as they use information that is not readily available to the human player, 
and second, they do not get better with experience. In the future, we would like to explore 
whether variations of these algorithms  can be used to play from the information conveyed by the  
 state of screen pixels as opposed to the RAM state, as well as exploring the potential uses of the novel
ideas presented for learning, in order to improve performance. 

%%% - Learning in same instance, across diff instances of same game, etc.
%%%% - Learning controllers vs. on-line planning, etc

%%% - Learning in same instance, across diff instances of same game, etc.
%%%% - Learning controllers vs. on-line planning, etc

\bibliographystyle{aaai}
\bibliography{control}

\end{document}
