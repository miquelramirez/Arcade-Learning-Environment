We have tested the $\IW(1)$ and \BFS~described in the previous Section
over $9$ games selected amongst those considered in the \emph{training} set
of \cite{bellemare:jair2013}, and those discussed in \cite{deep-mind-atari}. 
In Table~\ref{performance} the performance of both $\IW(1)$ 
and \BFS, characterized by the average score obtained over all episodes, the standard deviation
of the score obtained and time (in seconds) per lookahead, is compared with that of \UCT, 
\BRFS~and the baseline random action selection. Our experimental 
setup  follows that of \cite{bellemare:jair2013}, with three exceptions. First, rather than
considering $10$ episodes of each game, we have considered just $5$
due to time constraints, as each game can take as much as $40$ hours of 
CPU time. Second, while \UCT~is limited to perform up to $500$ base policy rollouts ($150,000$ frames
are to be simulated while performing the lookahead), both $\IW(1)$ and \BFS are limited to
simulate $100,000$ frames. Other \UCT~parameters such as the exploration constant $C$ and
the planning horizon $m$ have not been modified ($C=0.1$, $m=300$). Thirdly and last, both $\IW(1)$ 
and \BFS~are limited to a planning horizon $m=1500$, where one only an action is evaluated every $5$ frames, matching
the setup used during actual gameplay (an action is selected every $5$ frames, using a no-op
for the remaining $4$). We include the average score reported for \UCT~by \cite{bellemare:jair2013} 
in Table~\ref{performance} bracketed with parenthesis for reference. Also unchanged is the discount factor
used by all the algorithms ($\gamma=0.995$). All algorithms reuse the tree built between each 
lookahead by setting the root of the tree to the best child of the root in the previous one,
and deleting its siblings and their descendants, as \cite{bellemare:jair2013} does for \UCT. We note that 
this may have a different effect on algorithms where all branches have the same number of
generated nodes, as is the case of \BRFS, than in $\IW(1)$, \BFS~and \UCT, where the depth of
the branches is uneven.

Table~\ref{performance} shows that both $\IW(1)$ and \BFS~outperform by a wide margin 
\BRFS, which sometimes performs worse than the baseline, both in terms of average score attained and 
average time per call. \BRFS~rarely collects any reward to inform the choice of action at
the root node of the lookahead, as the depth of the \BRFS~search tree, having limited the number of
frames allowed to be simulated to $100,000$, corresponds to a lookahead of $0.3$ seconds ($20$ 
frames or $4$ nodes). Both $\IW(1)$ and \BFS~reach states up to $350$--$1500$ frames ahead 
($70$--$260$ nodes or $6$--$22$ seconds), and that results in collecting enough rewards so as to
inform the choice at the root in a way that much higher scores are attained. Interestingly, very often
neither $\IW(1)$ or \BFS~fully consume the number of simulation frames allocated, since many
states are pruned, in the case of $\IW(1)$, or relegated to the back of the open lists, in the 
case of \BFS, due to having a novelty of $0$ and the planning horizon being limited to $m$. 

We observe that in $4$ out of $9$ games, $\IW(1)$ outperforms \UCT~limited to the same number
of simulated frames, both in terms of average score attained and average time per call. \BFS~is
generally faster than $\IW(1)$, but only outperforms \UCT~in $2$ out of $9$ games (\Freeway and
\Seaquest) and $\IW(1)$ in $4$ out of $9$ games. Compared with the \UCT~results reported
in \cite{bellemare:jair2013}, that by limiting the number of rollouts allow a $50$\% more frames
to be simulated in each lookahead, $\IW(1)$ still outperforms \UCT~in $3$ out of $9$ games
(\Breakout, \Enduro~and \Freeway). As introduced in the previous Section, the order in which
$\IW(1)$ and \BFS~consider actions is randomized, since the novelty of a given state may
change depending on the order in which states are considered. We found this randomization to
have a significant effect in \Qbert~where not randomizing the order in which actions are considered
resulted in $\IW(1)$ being trapped in a loop. Last, we note that both $\IW(1)$ and \BFS~attain similar 
performance on \Freeway~as the baseline semi-random algorithm \emph{Perturb} 
from \cite{bellemare:jair2013}, which beated both \UCT~and the reinforcement learning algorithms reported.

We finish the discussion of the results presented in Table~\ref{performance} by observing
that $\IW(1)$ outperforms the average scores reported for the best algorithm reported in
\cite{deep-mind-atari} but in $4$ out of $7$ games, and \BFS~does so too in $5$ out of $7$
games.

We do not report $IW(2)$ in Table~\ref{performance} since we found its performance to
be very similar to that of \BRFS~in some preliminary tests. When considering $2$--tuples
of state variables to evaluate the novelty of states, it tended to result in a lookahead
$5$ times deeper than those resulting from applying \BRFS~, but still far too shallow so
as to compete with the other planning algorithms considered.

% On Table~\ref{performance}, we can observe the performance of each
% algorithm on terms of average score with standard deviation, and
% average time per lookahead in seconds. The Random agent don't include
% average time as it just chooses an action randomly. BRFS outperform
% the Random agent in 7 out of 9 domains. BRFS rarely finds any reward,
% turning itself mostly into a random agent, but exploring 100k nodes up
% to an average depth of 4. When BRFS is turned int IW1 using novelty to
% explore the search space, the jump on performance is important. IW1 in
% most of the cases does not completely use its budget, and is able to
% achieve depths of 70 to 260 depending on the game, which first
% explains how IW1 is able to achieve a performance that is closer to
% UCT than to BRFS. In 4 out of 9 tested games it is the best
% performer. Remarkably, is the first domain independent solver that
% shows good performance on Freeway, where rewards are very sparse and
% deep in the search tree. BFS outperforms IW1 and UCT in 4 and 2
% domains respectively.  Both IW1 and BFS performance in terms of score
% is close to UCT, but they are some times one order of magnitud
% faster. The speedup is due to the bounded space in IW1, and to the
% fact that in some games, both IW1 and BFS are able to reuse an
% important part of their trees between lookaheads.
% 
% The novelty of states in IW1 and BFS may depend on the order that
% nodes are generated. Therefore, every time a node is expanded, we
% randomized the order in which nodes get genreated. Randomizing the
% order of expansion is also applied to BRFS, but has no impact on its
% performance.
% 
% IW2 is not included in the table due to its low performance, which is
% closer to BRFS than to the other algorithms. This is mainly due to the
% huge number of states that are novel in this algorith, thus,
% achieveing insufficient depths to gather some reward.


%  ** mention also ideas that have been tried and didn't work (IW2?,
%  discount, randomization, ..)
% 

% The algorithms IW1 and BFS are tested over 9 games selected from those
% considered in the training set in \cite{bellemare:jair}, and those
% considered in \cite{deep-mind-atari}. Each game has 5 runs, each with
% a different random seed $0,\ldots,4$.  We compare the performance of
% IW1 and BFS with UCT and BRFS. We also include results for a baseline random
% agent. We use the same parameters in the literature, all algorithms use a discount factor of 0.999, and each action
% simulates 5 frames. Each algorithm has a budget on the number of nodes
% it can generate per lookahead. BRFS, IW1 and BFS have a maximum budget
% of 100k. For UCT we use the same parameters as in
% \cite{bellemare:jair}, 0.1 exploration rate, and 500 rollouts of a maximum depth of 300 steps,
% which gives a maximum budget of 150k nodes. We tested UCT with a fix
% budget of 100k and max depth 300, but resulted in practically the same
% results. All algorithms reuse the tree built between lookaheads by
% setting the new root of the tree to the node of the selected action,
% and deleting the other branches. Note that this may have a different
% effect on algorithms were all branches have the same number of
% generated nodes, ex. BRFS, than those that may focus on different
% areas of the tree, ex. IW1, BFS, UCT. 

% On Table~\ref{performance}, we can observe the performance of each
% algorithm on terms of average score with standard deviation, and
% average time per lookahead in seconds. The Random agent don't include
% average time as it just chooses an action randomly. BRFS outperform
% the Random agent in 7 out of 9 domains. BRFS rarely finds any reward,
% turning itself mostly into a random agent, but exploring 100k nodes up
% to an average depth of 4. When BRFS is turned int IW1 using novelty to
% explore the search space, the jump on performance is important. IW1 in
% most of the cases does not completely use its budget, and is able to
% achieve depths of 70 to 260 depending on the game, which first
% explains how IW1 is able to achieve a performance that is closer to
% UCT than to BRFS. In 4 out of 9 tested games it is the best
% performer. Remarkably, is the first domain independent solver that
% shows good performance on Freeway, where rewards are very sparse and
% deep in the search tree. BFS outperforms IW1 and UCT in 4 and 2
% domains respectively.  Both IW1 and BFS performance in terms of score
% is close to UCT, but they are some times one order of magnitud
% faster. The speedup is due to the bounded space in IW1, and to the
% fact that in some games, both IW1 and BFS are able to reuse an
% important part of their trees between lookaheads.

 
